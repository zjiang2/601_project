{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import pickle\n",
    "import nltk\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.regularizers import l1,l2, L1L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 273958\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in tweets\n",
    "with open('../data/pickled_tweets/home_2019_3daysback_nolikecriterion.pkl', 'rb') as f:\n",
    "    home_2019_tweets = pickle.load(f)\n",
    "with open('../data/pickled_tweets/away_2019_3daysback_nolikecriterion.pkl', 'rb') as f:\n",
    "    away_2019_tweets = pickle.load(f)\n",
    "with open('../data/pickled_tweets/home_2020_3daysback_nolikecriterion.pkl', 'rb') as f:\n",
    "    home_2020_tweets = pickle.load(f)\n",
    "with open('../data/pickled_tweets/away_2020_3daysback_nolikecriterion.pkl', 'rb') as f:\n",
    "    away_2020_tweets = pickle.load(f)\n",
    "\n",
    "# load in nfl data\n",
    "s2020 = pd.read_csv('../data/season_data/2020_all_data.csv', index_col=0)\n",
    "s2019 = pd.read_csv('../data/season_data/2019_all_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a list of stopwords to remove.\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# lemmatize function\n",
    "def lemmatize(sentence, include_stopwords=False):\n",
    "    if include_stopwords:\n",
    "        return [WordNetLemmatizer().lemmatize(word) for word in sentence]\n",
    "    return [WordNetLemmatizer().lemmatize(word) for word in sentence if word not in stopwords]\n",
    "\n",
    "# preprocess the tweets - remove punctuation and lemmatize\n",
    "def preprocess(tweets):\n",
    "    for i in range(len(tweets)):\n",
    "        for j in range(len(tweets[i])):\n",
    "            tweets[i][j] = re.sub('[^a-zA-Z]',' ',tweets[i][j]).split()\n",
    "            tweets[i][j] = lemmatize(tweets[i][j])\n",
    "\n",
    "preprocess(home_2019_tweets)\n",
    "preprocess(away_2019_tweets)\n",
    "preprocess(home_2020_tweets)\n",
    "preprocess(away_2020_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def vectorize_list(list_of_tweets, corpus):\n",
    "    num_tweets = len(list_of_tweets)\n",
    "    with_repeats = [item for sublist in list_of_tweets for item in sublist]\n",
    "    counts = dict(Counter(with_repeats))\n",
    "    to_return = []\n",
    "    for key in corpus:\n",
    "        num = np.log(1+counts[key]) if key in counts else 0\n",
    "        to_return.append(num)\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count unigrams\n",
    "def count_unigrams(tweet,corpus):\n",
    "    for word in tweet:\n",
    "        if word in corpus:\n",
    "            corpus[word] += 1\n",
    "        else:\n",
    "            corpus[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the home/away corpus\n",
    "home_corpus = {}\n",
    "away_corpus = {}\n",
    "for tweets in home_2019_tweets:\n",
    "    for tw in tweets:\n",
    "        count_unigrams(tw,home_corpus)\n",
    "\n",
    "for tweets in away_2019_tweets:\n",
    "    for tw in tweets:\n",
    "        count_unigrams(tw,away_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words with <= 2 characters\n",
    "for key in list(home_corpus.keys()):\n",
    "    if len(key) <= 2:\n",
    "        del home_corpus[key]\n",
    "\n",
    "for key in list(away_corpus.keys()):\n",
    "    if len(key) <= 2:\n",
    "        del away_corpus[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of tweets for all home and away teams\n",
    "n_home_tweets = sum([len(game_tweets) for game_tweets in home_2019_tweets])\n",
    "n_away_tweets = sum([len(game_tweets) for game_tweets in away_2019_tweets])\n",
    "\n",
    "# get the unigrams that appear in at least 0.1% of home/away tweets\n",
    "home_top_grams = [word for word in home_corpus if home_corpus[word] > n_home_tweets*0.001]\n",
    "away_top_grams = [word for word in away_corpus if away_corpus[word] > n_away_tweets*0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN SET\n",
    "#Create lists of vectors for home, away games\n",
    "num_h_tweets = [vectorize_list(game, home_top_grams) for game in home_2019_tweets]\n",
    "num_a_tweets = [vectorize_list(game, away_top_grams) for game in away_2019_tweets]\n",
    "\n",
    "#Turn into arrays\n",
    "home_vecs = np.array(num_h_tweets)\n",
    "away_vecs = np.array(num_a_tweets)\n",
    "\n",
    "#Concatenate home, away to form input matrix.\n",
    "X_train = np.concatenate([home_vecs, away_vecs], axis=1)\n",
    "\n",
    "# TEST SET\n",
    "#Create lists of vectors for home, away games\n",
    "num_h_tweets = [vectorize_list(game, home_top_grams) for game in home_2020_tweets]\n",
    "num_a_tweets = [vectorize_list(game, away_top_grams) for game in away_2020_tweets]\n",
    "\n",
    "#Turn into arrays\n",
    "home_vecs = np.array(num_h_tweets)\n",
    "away_vecs = np.array(num_a_tweets)\n",
    "\n",
    "#Concatenate home, away to form input matrix.\n",
    "X_test = np.concatenate([home_vecs, away_vecs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN and TEST outcomes\n",
    "Y_train = np.array(s2019[\"Home Win\"])\n",
    "Y_test = np.array(s2020[\"Home Win\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57421875\n",
      "0.56640625\n",
      "0.625\n",
      "0.51953125\n",
      "0.55859375\n",
      "0.5703125\n"
     ]
    }
   ],
   "source": [
    "# Basic classifiers on full data\n",
    "lr = LogisticRegression(penalty=\"l1\",solver=\"liblinear\",C=0.1)\n",
    "lr.fit(X_train, Y_train)\n",
    "print(lr.score(X_test, Y_test))\n",
    "\n",
    "lrl1 = LogisticRegressionCV(cv=5,penalty=\"l1\",solver=\"liblinear\",\n",
    "                            max_iter=1000).fit(X_train, Y_train)\n",
    "lrl1.fit(X_train, Y_train)\n",
    "print(lrl1.score(X_test, Y_test))\n",
    "\n",
    "lrl2 = LogisticRegressionCV(cv=5,penalty=\"l2\",solver=\"liblinear\",\n",
    "                            max_iter=1000).fit(X_train, Y_train)\n",
    "lrl2.fit(X_train, Y_train)\n",
    "print(lrl2.score(X_test, Y_test))\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators=200)\n",
    "ada.fit(X_train, Y_train)\n",
    "print(ada.score(X_test, Y_test))\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400)\n",
    "rf.fit(X_train, Y_train)\n",
    "print(rf.score(X_test, Y_test))\n",
    "\n",
    "gnb = GaussianNB().fit(X_train, Y_train)\n",
    "gnb.fit(X_train, Y_train)\n",
    "print(gnb.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse L1 Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57421875\n"
     ]
    }
   ],
   "source": [
    "# Basic classifiers on full data\n",
    "lr = LogisticRegression(penalty=\"l1\",solver=\"liblinear\",C=0.1)\n",
    "lr.fit(X_train, Y_train)\n",
    "print(lr.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(lr.coef_ != 0)[1]) #sparsity of lr coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fox', 'ram', 'que', 'doubt', 'yankee', 'mlb', 'scored', 'adam',\n",
       "       'offense', 'owner', 'tnf', 'tournament', 'snap', 'johnson', 'pro',\n",
       "       'del', 'beginning', 'florida', 'prime', 'gordon', 'bruce',\n",
       "       'hamstring', 'dallascowboys', 'drew', 'tennessee', 'texas', 'bucs',\n",
       "       'ravensnation', 'wpmoychallenge', 'que', 'thursday', 'september',\n",
       "       'sam', 'shot', 'mike', 'rush', 'senior', 'dallascowboys', 'falcon',\n",
       "       'odds', 'colt', 'robert', 'state', 'share', 'chiefskingdom',\n",
       "       'november', 'ankle', 'tune', 'harris', 'saint', 'obj', 'cold',\n",
       "       'seahawks', 'raven', 'halloween', 'wilson', 'whodey',\n",
       "       'raidernation'], dtype='<U19')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_grams = np.array(home_top_grams + away_top_grams)\n",
    "all_grams[np.where(lr.coef_ != 0)[1]] #grams corresponding to nonzero entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 374,  388,  390,  393,  408,  409,  424,  428,  453,  577,  586,\n",
       "        687,  799,  818,  827,  870,  978, 1073, 1100, 1108, 1229, 1236,\n",
       "       1249, 1263, 1264, 1363, 1407, 1463, 1507, 1527, 1620, 1823, 1873,\n",
       "       1887, 1890, 1905, 1947, 2014, 2024, 2069, 2165, 2199, 2276, 2376,\n",
       "       2402, 2426, 2487, 2509, 2536, 2651, 2693, 2712, 2787, 2805, 2807,\n",
       "       2877, 2933, 3020])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(lr.coef_ != 0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "signs = np.sign(np.take(lr.coef_, np.where(lr.coef_ != 0)[1])).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_away = np.where(np.where(lr.coef_ != 0)[1] > len(home_top_grams), 'A', 'H').reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['fox', 'ram', 'que', 'doubt', 'yankee', 'mlb', 'scored', 'adam',\n",
       "       'offense', 'owner', 'tnf', 'tournament', 'snap', 'johnson', 'pro',\n",
       "       'del', 'beginning', 'florida', 'prime', 'gordon', 'bruce',\n",
       "       'hamstring', 'dallascowboys', 'drew', 'tennessee', 'texas', 'bucs',\n",
       "       'ravensnation', 'wpmoychallenge', 'que', 'thursday', 'september',\n",
       "       'sam', 'shot', 'mike', 'rush', 'senior', 'dallascowboys', 'falcon',\n",
       "       'odds', 'colt', 'robert', 'state', 'share', 'chiefskingdom',\n",
       "       'november', 'ankle', 'tune', 'harris', 'saint', 'obj', 'cold',\n",
       "       'seahawks', 'raven', 'halloween', 'wilson', 'whodey',\n",
       "       'raidernation'], dtype='<U19')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_grams[np.where(lr.coef_ != 0)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['fox', '1.0', '374', 'H'],\n",
       "       ['ram', '1.0', '388', 'H'],\n",
       "       ['que', '1.0', '390', 'H'],\n",
       "       ['doubt', '-1.0', '393', 'H'],\n",
       "       ['yankee', '-1.0', '408', 'H'],\n",
       "       ['mlb', '-1.0', '409', 'H'],\n",
       "       ['scored', '1.0', '424', 'H'],\n",
       "       ['adam', '1.0', '428', 'H'],\n",
       "       ['offense', '1.0', '453', 'H'],\n",
       "       ['owner', '-1.0', '577', 'H'],\n",
       "       ['tnf', '1.0', '586', 'H'],\n",
       "       ['tournament', '-1.0', '687', 'H'],\n",
       "       ['snap', '1.0', '799', 'H'],\n",
       "       ['johnson', '1.0', '818', 'H'],\n",
       "       ['pro', '1.0', '827', 'H'],\n",
       "       ['del', '1.0', '870', 'H'],\n",
       "       ['beginning', '1.0', '978', 'H'],\n",
       "       ['florida', '-1.0', '1073', 'H'],\n",
       "       ['prime', '1.0', '1100', 'H'],\n",
       "       ['gordon', '1.0', '1108', 'H'],\n",
       "       ['bruce', '-1.0', '1229', 'H'],\n",
       "       ['hamstring', '-1.0', '1236', 'H'],\n",
       "       ['dallascowboys', '1.0', '1249', 'H'],\n",
       "       ['drew', '1.0', '1263', 'H'],\n",
       "       ['tennessee', '-1.0', '1264', 'H'],\n",
       "       ['texas', '1.0', '1363', 'H'],\n",
       "       ['bucs', '-1.0', '1407', 'H'],\n",
       "       ['ravensnation', '1.0', '1463', 'H'],\n",
       "       ['wpmoychallenge', '-1.0', '1507', 'H'],\n",
       "       ['que', '-1.0', '1527', 'A'],\n",
       "       ['thursday', '-1.0', '1620', 'A'],\n",
       "       ['september', '-1.0', '1823', 'A'],\n",
       "       ['sam', '1.0', '1873', 'A'],\n",
       "       ['shot', '1.0', '1887', 'A'],\n",
       "       ['mike', '-1.0', '1890', 'A'],\n",
       "       ['rush', '-1.0', '1905', 'A'],\n",
       "       ['senior', '1.0', '1947', 'A'],\n",
       "       ['dallascowboys', '1.0', '2014', 'A'],\n",
       "       ['falcon', '-1.0', '2024', 'A'],\n",
       "       ['odds', '1.0', '2069', 'A'],\n",
       "       ['colt', '1.0', '2165', 'A'],\n",
       "       ['robert', '-1.0', '2199', 'A'],\n",
       "       ['state', '1.0', '2276', 'A'],\n",
       "       ['share', '-1.0', '2376', 'A'],\n",
       "       ['chiefskingdom', '-1.0', '2402', 'A'],\n",
       "       ['november', '1.0', '2426', 'A'],\n",
       "       ['ankle', '-1.0', '2487', 'A'],\n",
       "       ['tune', '-1.0', '2509', 'A'],\n",
       "       ['harris', '1.0', '2536', 'A'],\n",
       "       ['saint', '-1.0', '2651', 'A'],\n",
       "       ['obj', '1.0', '2693', 'A'],\n",
       "       ['cold', '1.0', '2712', 'A'],\n",
       "       ['seahawks', '-1.0', '2787', 'A'],\n",
       "       ['raven', '-1.0', '2805', 'A'],\n",
       "       ['halloween', '1.0', '2807', 'A'],\n",
       "       ['wilson', '-1.0', '2877', 'A'],\n",
       "       ['whodey', '1.0', '2933', 'A'],\n",
       "       ['raidernation', '1.0', '3020', 'A']], dtype='<U32')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([all_grams[np.where(lr.coef_ != 0)[1]].reshape(-1,1), signs, np.where(lr.coef_ != 0)[1].reshape(-1,1), home_away], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validated L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1958"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(lrl1.coef_ != 0)[1]) #sparsity of lr coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['drew', 'force', 'moore', 'surgery', 'ranking', 'participant',\n",
       "       'mack', 'professional', 'michigan', 'cold', 'value', 'anderson',\n",
       "       'harris', 'turnover', 'whodey', 'ground', 'tnf', 'november',\n",
       "       'ravensnation', 'beginning'], dtype='<U19')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_positives = np.argpartition(lrl1.coef_.reshape(-1), -20)[-20:]\n",
    "all_grams[max_positives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['drew', '1.0', '1263', 'H'],\n",
       "       ['force', '1.0', '1259', 'H'],\n",
       "       ['moore', '1.0', '1260', 'H'],\n",
       "       ['surgery', '1.0', '2245', 'A'],\n",
       "       ['ranking', '1.0', '936', 'H'],\n",
       "       ['participant', '1.0', '2544', 'A'],\n",
       "       ['mack', '1.0', '1842', 'A'],\n",
       "       ['professional', '1.0', '1027', 'H'],\n",
       "       ['michigan', '1.0', '2480', 'A'],\n",
       "       ['cold', '1.0', '2712', 'A'],\n",
       "       ['value', '1.0', '2695', 'A'],\n",
       "       ['anderson', '1.0', '2421', 'A'],\n",
       "       ['harris', '1.0', '2536', 'A'],\n",
       "       ['turnover', '1.0', '2716', 'A'],\n",
       "       ['whodey', '1.0', '2933', 'A'],\n",
       "       ['ground', '1.0', '1352', 'H'],\n",
       "       ['tnf', '1.0', '586', 'H'],\n",
       "       ['november', '1.0', '2426', 'A'],\n",
       "       ['ravensnation', '1.0', '1463', 'H'],\n",
       "       ['beginning', '1.0', '978', 'H']], dtype='<U32')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signs = np.sign(np.take(lrl1.coef_, max_positives)).reshape(-1,1)\n",
    "home_away = np.where(max_positives > len(home_top_grams), 'A', 'H').reshape(-1,1)\n",
    "np.concatenate([all_grams[max_positives].reshape(-1,1), signs, max_positives.reshape(-1,1), home_away], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['terrible', 'tip', 'turned', 'duke', 'loud', 'ravensnation',\n",
       "       'tyler', 'goravens', 'ness', 'preseason', 'lmao', 'county', 'ward',\n",
       "       'sundberg', 'kcchiefs', 'aigo', 'cat', 'watt', 'september',\n",
       "       'hater'], dtype='<U19')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_negatives = np.argpartition(-1*lrl1.coef_.reshape(-1), -20)[-20:]\n",
    "all_grams[max_negatives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['terrible', '-1.0', '2793', 'A'],\n",
       "       ['tip', '-1.0', '826', 'H'],\n",
       "       ['turned', '-1.0', '2434', 'A'],\n",
       "       ['duke', '-1.0', '1950', 'A'],\n",
       "       ['loud', '-1.0', '2216', 'A'],\n",
       "       ['ravensnation', '-1.0', '2928', 'A'],\n",
       "       ['tyler', '-1.0', '2381', 'A'],\n",
       "       ['goravens', '-1.0', '2926', 'A'],\n",
       "       ['ness', '-1.0', '2929', 'A'],\n",
       "       ['preseason', '-1.0', '997', 'H'],\n",
       "       ['lmao', '-1.0', '1051', 'H'],\n",
       "       ['county', '-1.0', '1026', 'H'],\n",
       "       ['ward', '-1.0', '2880', 'A'],\n",
       "       ['sundberg', '-1.0', '1465', 'H'],\n",
       "       ['kcchiefs', '-1.0', '2920', 'A'],\n",
       "       ['aigo', '-1.0', '3028', 'A'],\n",
       "       ['cat', '-1.0', '788', 'H'],\n",
       "       ['watt', '-1.0', '2817', 'A'],\n",
       "       ['september', '-1.0', '1823', 'A'],\n",
       "       ['hater', '-1.0', '236', 'H']], dtype='<U32')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signs = np.sign(np.take(lrl1.coef_, max_negatives)).reshape(-1,1)\n",
    "home_away = np.where(max_negatives > len(home_top_grams), 'A', 'H').reshape(-1,1)\n",
    "np.concatenate([all_grams[max_negatives].reshape(-1,1), signs, max_negatives.reshape(-1,1), home_away], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validated L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(np.where(lrl2.coef_ != 0)[1]) #sparsity of lr coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_positives = np.argpartition(lrl2.coef_.reshape(-1), -20)[-20:]\n",
    "all_grams = np.array(home_top_grams + away_top_grams)\n",
    "all_grams[max_positives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signs = np.sign(np.take(lrl2.coef_, max_positives)).reshape(-1,1)\n",
    "home_away = np.where(max_positives > len(home_top_grams), 'A', 'H').reshape(-1,1)\n",
    "np.concatenate([all_grams[max_positives].reshape(-1,1), signs, max_positives.reshape(-1,1), home_away], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_negatives = np.argpartition(-1*lrl2.coef_.reshape(-1), -20)[-20:]\n",
    "all_grams = np.array(home_top_grams + away_top_grams)\n",
    "all_grams[max_negatives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signs = np.sign(np.take(lrl2.coef_, max_negatives)).reshape(-1,1)\n",
    "home_away = np.where(max_negatives > len(home_top_grams), 'A', 'H').reshape(-1,1)\n",
    "np.concatenate([all_grams[max_negatives], signs, max_negatives.reshape(-1,1), home_away], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
