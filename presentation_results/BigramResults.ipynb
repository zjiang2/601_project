{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import pickle\n",
    "import nltk\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.regularizers import l1,l2, L1L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 273958\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in tweets\n",
    "with open('../data/pickled_tweets/home_2019_3daysback_nolikecriterion.pkl', 'rb') as f:\n",
    "    home_2019_tweets = pickle.load(f)\n",
    "with open('../data/pickled_tweets/away_2019_3daysback_nolikecriterion.pkl', 'rb') as f:\n",
    "    away_2019_tweets = pickle.load(f)\n",
    "with open('../data/pickled_tweets/home_2020_3daysback_nolikecriterion.pkl', 'rb') as f:\n",
    "    home_2020_tweets = pickle.load(f)\n",
    "with open('../data/pickled_tweets/away_2020_3daysback_nolikecriterion.pkl', 'rb') as f:\n",
    "    away_2020_tweets = pickle.load(f)\n",
    "\n",
    "# load in nfl data\n",
    "s2020 = pd.read_csv('../data/season_data/2020_all_data.csv', index_col=0)\n",
    "s2019 = pd.read_csv('../data/season_data/2019_all_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = stopwords.words('english')\n",
    "\n",
    "# lemmatize function\n",
    "def lemmatize(sentence, include_stopwords=False):\n",
    "    if include_stopwords:\n",
    "        return [WordNetLemmatizer().lemmatize(word) for word in sentence]\n",
    "    return [WordNetLemmatizer().lemmatize(word) for word in sentence if word not in sw]\n",
    "\n",
    "# preprocess the tweets - remove punctuation and lemmatize\n",
    "def preprocess(tweets):\n",
    "    for i in range(len(tweets)):\n",
    "        for j in range(len(tweets[i])):\n",
    "            tweets[i][j] = re.sub('[^a-zA-Z]',' ',tweets[i][j]).split()\n",
    "            tweets[i][j] = lemmatize(tweets[i][j])\n",
    "\n",
    "preprocess(home_2019_tweets)\n",
    "preprocess(away_2019_tweets)\n",
    "preprocess(home_2020_tweets)\n",
    "preprocess(away_2020_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def vectorize_list(list_of_tweets, corpus):\n",
    "    num_tweets = len(list_of_tweets)\n",
    "    lol = [nltk.bigrams(x) for x in list_of_tweets]\n",
    "    with_repeats = [item for sublist in lol for item in sublist]\n",
    "    counts = dict(Counter(with_repeats))\n",
    "    to_return = []\n",
    "    for key in corpus:\n",
    "        num = np.log(1+counts[key]) if key in counts else 0\n",
    "        to_return.append(num)\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count bigrams\n",
    "def count_bigrams(tweet,corpus):\n",
    "    bigramized_tweet = nltk.bigrams(tweet)\n",
    "    for bigram in bigramized_tweet:\n",
    "        if bigram in corpus:\n",
    "            corpus[bigram] += 1\n",
    "        else:\n",
    "            corpus[bigram] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the home/away corpus\n",
    "home_corpus = {}\n",
    "away_corpus = {}\n",
    "for tweets in home_2019_tweets:\n",
    "    for tw in tweets:\n",
    "        count_bigrams(tw,home_corpus)\n",
    "\n",
    "for tweets in away_2019_tweets:\n",
    "    for tw in tweets:\n",
    "        count_bigrams(tw,away_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of tweets for all home and away teams\n",
    "n_home_tweets = sum([len(game_tweets) for game_tweets in home_2019_tweets])\n",
    "n_away_tweets = sum([len(game_tweets) for game_tweets in away_2019_tweets])\n",
    "\n",
    "# get the unigrams that appear in at least 0.1% of home/away tweets\n",
    "home_top_grams = [word for word in home_corpus if home_corpus[word] > n_home_tweets*0.0001]\n",
    "away_top_grams = [word for word in away_corpus if away_corpus[word] > n_away_tweets*0.0001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN SET\n",
    "#Create lists of vectors for home, away games\n",
    "num_h_tweets = [vectorize_list(game, home_top_grams) for game in home_2019_tweets]\n",
    "num_a_tweets = [vectorize_list(game, away_top_grams) for game in away_2019_tweets]\n",
    "\n",
    "#Turn into arrays\n",
    "home_vecs = np.array(num_h_tweets)\n",
    "away_vecs = np.array(num_a_tweets)\n",
    "\n",
    "#Concatenate home, away to form input matrix.\n",
    "X_train = np.concatenate([home_vecs, away_vecs], axis=1)\n",
    "\n",
    "# TEST SET\n",
    "#Create lists of vectors for home, away games\n",
    "num_h_tweets = [vectorize_list(game, home_top_grams) for game in home_2020_tweets]\n",
    "num_a_tweets = [vectorize_list(game, away_top_grams) for game in away_2020_tweets]\n",
    "\n",
    "#Turn into arrays\n",
    "home_vecs = np.array(num_h_tweets)\n",
    "away_vecs = np.array(num_a_tweets)\n",
    "\n",
    "#Concatenate home, away to form input matrix.\n",
    "X_test = np.concatenate([home_vecs, away_vecs], axis=1)\n",
    "\n",
    "# TRAIN and TEST outcomes\n",
    "Y_train = np.array(s2019[\"Home Win\"])\n",
    "Y_test = np.array(s2020[\"Home Win\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.57421875\n",
      "0.60546875\n",
      "0.61328125\n",
      "0.5625\n",
      "0.58984375\n",
      "0.53125\n"
     ]
    }
   ],
   "source": [
    "# Basic classifiers on full data\n",
    "lr = LogisticRegression(penalty=\"l1\",solver=\"liblinear\",C=0.1)\n",
    "lr.fit(X_train, Y_train)\n",
    "print(lr.score(X_test, Y_test))\n",
    "\n",
    "lrl1 = LogisticRegressionCV(cv=5,penalty=\"l1\",solver=\"liblinear\",\n",
    "                            max_iter=1000).fit(X_train, Y_train)\n",
    "lrl1.fit(X_train, Y_train)\n",
    "print(lrl1.score(X_test, Y_test))\n",
    "\n",
    "lrl2 = LogisticRegressionCV(cv=5,penalty=\"l2\",solver=\"liblinear\",\n",
    "                            max_iter=1000).fit(X_train, Y_train)\n",
    "lrl2.fit(X_train, Y_train)\n",
    "print(lrl2.score(X_test, Y_test))\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators=200)\n",
    "ada.fit(X_train, Y_train)\n",
    "print(ada.score(X_test, Y_test))\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400)\n",
    "rf.fit(X_train, Y_train)\n",
    "print(rf.score(X_test, Y_test))\n",
    "\n",
    "gnb = GaussianNB().fit(X_train, Y_train)\n",
    "gnb.fit(X_train, Y_train)\n",
    "print(gnb.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 2s 131ms/step - loss: 3.1070 - accuracy: 0.5100\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 1s 127ms/step - loss: 2.4411 - accuracy: 0.3929\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 1s 128ms/step - loss: 2.3236 - accuracy: 0.4968\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 1s 128ms/step - loss: 2.2081 - accuracy: 0.5838\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 1s 130ms/step - loss: 2.1155 - accuracy: 0.5266\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 1s 130ms/step - loss: 1.9268 - accuracy: 0.7282\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 1s 125ms/step - loss: 1.6889 - accuracy: 0.8383\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 1s 129ms/step - loss: 1.3470 - accuracy: 0.9569\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 1s 142ms/step - loss: 1.1723 - accuracy: 0.9927\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 1s 137ms/step - loss: 1.0731 - accuracy: 0.9954\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 2.3456 - accuracy: 0.5938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.34562087059021, 0.59375]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg=1e-5\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='tanh', kernel_regularizer=L1L2(reg), input_dim=X_train.shape[1]))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(512, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(256, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(128, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(64, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=10, verbose=True)\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPZElEQVR4nO3df6xfd13H8efL1hUDMrC7EtJ23GKrpmACWrp/gBCXYSfRQuygI9GazBQiTTTEhGLinA0mG1GmifNHtYujit1SRG+ykmky469g7S1MR7dU70rJWid07TKdWkbZ2z/uGX79cu/u6b3329t+7vOR3PScz/l8b9+fnez1/Xw/53tOU1VIktr1bUtdgCRptAx6SWqcQS9JjTPoJalxBr0kNW7lUhcw7Lrrrqvx8fGlLkOSrirHjh17uqrGZjp2xQX9+Pg4k5OTS12GJF1Vknx5tmMu3UhS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuOuuDtjF2p8z4Nz9jl157suQyWSdGVwRi9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIa1yvok2xNciLJVJI9MxxfleT+7viRJONd+3iS/0nySPfzu4tcvyRpDnN+jz7JCuAe4CbgNHA0yURVPTbQ7TbgmarakGQHcBfwvu7YE1X1psUtW5LUV58Z/RZgqqpOVtXzwEFg21CfbcB93fYh4MYkWbwyJUnz1Sfo1wBPDuyf7tpm7FNVF4FngdXdsfVJvpDkr5O8baa/IMmuJJNJJs+ePXtJA5AkvbRRX4x9Cri+qt4MfBj4VJJXDneqqn1VtbmqNo+NzfiPmEuS5qlP0J8B1g3sr+3aZuyTZCVwLXCuqr5WVecAquoY8ATwvQstWpLUX5+gPwpsTLI+yTXADmBiqM8EsLPb3g48XFWVZKy7mEuS1wMbgZOLU7okqY85v3VTVReT7AYeAlYA91bV8SR7gcmqmgD2AweSTAHnmX4zAHg7sDfJ14EXgA9W1flRDESSNLNejymuqsPA4aG22we2LwC3zPC6TwOfXmCNkqQF8M5YSWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1LheQZ9ka5ITSaaS7Jnh+Kok93fHjyQZHzp+fZLnkvzCItUtSeppzqBPsgK4B7gZ2ATcmmTTULfbgGeqagNwN3DX0PFPAJ9deLmSpEvVZ0a/BZiqqpNV9TxwENg21GcbcF+3fQi4MUkAkrwb+BJwfFEqliRdkj5BvwZ4cmD/dNc2Y5+qugg8C6xO8grgI8CvvNRfkGRXkskkk2fPnu1buySph1FfjL0DuLuqnnupTlW1r6o2V9XmsbGxEZckScvLyh59zgDrBvbXdm0z9TmdZCVwLXAOuAHYnuTjwKuAF5JcqKrfWmjhkqR++gT9UWBjkvVMB/oO4P1DfSaAncDngO3Aw1VVwNte7JDkDuA5Q16SLq85g76qLibZDTwErADurarjSfYCk1U1AewHDiSZAs4z/WYgSboC9JnRU1WHgcNDbbcPbF8Abpnjd9wxj/okSQvknbGS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxK5e6gKU2vufBXv1O3fmuEVciSaPhjF6SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUuF5Bn2RrkhNJppLsmeH4qiT3d8ePJBnv2rckeaT7+ack71nk+iVJc5gz6JOsAO4BbgY2Abcm2TTU7TbgmaraANwN3NW1fxHYXFVvArYCv5dk2d+NK0mXU58Z/RZgqqpOVtXzwEFg21CfbcB93fYh4MYkqar/rqqLXfvLgFqMoiVJ/fUJ+jXAkwP7p7u2Gft0wf4ssBogyQ1JjgOPAh8cCP5vSrIryWSSybNnz176KCRJsxr5xdiqOlJVbwDeAnw0yctm6LOvqjZX1eaxsbFRlyRJy0qfoD8DrBvYX9u1zdinW4O/Fjg32KGqHgeeA94432IlSZeuT9AfBTYmWZ/kGmAHMDHUZwLY2W1vBx6uqupesxIgyeuA7wdOLUrlkqRe5vwGTFVdTLIbeAhYAdxbVceT7AUmq2oC2A8cSDIFnGf6zQDgrcCeJF8HXgB+tqqeHsVAJEkz6/VVx6o6DBweart9YPsCcMsMrzsAHFhgjZKkBfDOWElqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mN8197ukTjex7s1e/Une8acSWS1I8zeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhrn8+hHzOfXS1pqzuglqXEGvSQ1zqCXpMb1CvokW5OcSDKVZM8Mx1club87fiTJeNd+U5JjSR7t/vzhRa5fkjSHOYM+yQrgHuBmYBNwa5JNQ91uA56pqg3A3cBdXfvTwI9V1Q8AO4EDi1W4JKmfPjP6LcBUVZ2squeBg8C2oT7bgPu67UPAjUlSVV+oqn/r2o8D35Fk1WIULknqp0/QrwGeHNg/3bXN2KeqLgLPAquH+vwE8Pmq+trwX5BkV5LJJJNnz57tW7skqYfLcjE2yRuYXs75wEzHq2pfVW2uqs1jY2OXoyRJWjb6BP0ZYN3A/tqubcY+SVYC1wLnuv21wGeAn6qqJxZasCTp0vS5M/YosDHJeqYDfQfw/qE+E0xfbP0csB14uKoqyauAB4E9VfX3i1Z14/rcTeudtJL6mnNG36257wYeAh4HHqiq40n2Jvnxrtt+YHWSKeDDwItfwdwNbABuT/JI9/Pdiz4KSdKsej3rpqoOA4eH2m4f2L4A3DLD6z4GfGyBNWoOPk9H0kvxzlhJapxBL0mN8zHFy5BLPdLy4oxekhpn0EtS41y60Zxc6pGubga9RsI3B+nK4dKNJDXOoJekxhn0ktQ4g16SGufFWF0RvHgrjY4zeklqnDN6XZX8BCD154xekhrnjF7Lhv9yl5YrZ/SS1DiDXpIa59KNNAsv+KoVzuglqXHO6KVF4icAXamc0UtS45zRS0vETwC6XAx66SrivQCaD5duJKlxzuilhrk8JDDoJQ241DcG30iuDi7dSFLjnNFLuqz8FHD5OaOXpMb1mtEn2Qr8JrAC+IOqunPo+Crgk8APAeeA91XVqSSrgUPAW4A/rKrdi1m8pPb5CWDh5gz6JCuAe4CbgNPA0SQTVfXYQLfbgGeqakOSHcBdwPuAC8AvAW/sfiRppHxj+FZ9lm62AFNVdbKqngcOAtuG+mwD7uu2DwE3JklV/VdV/R3TgS9JWgJ9lm7WAE8O7J8GbpitT1VdTPIssBp4uk8RSXYBuwCuv/76Pi+RpEVzqXccX22fGq6Ii7FVta+qNlfV5rGxsaUuR5Ka0mdGfwZYN7C/tmubqc/pJCuBa5m+KCtJy95SfwLoM6M/CmxMsj7JNcAOYGKozwSws9veDjxcVbV4ZUqS5mvOGX235r4beIjpr1feW1XHk+wFJqtqAtgPHEgyBZxn+s0AgCSngFcC1yR5N/DOoW/sSJJGqNf36KvqMHB4qO32ge0LwC2zvHZ8AfVJkhboirgYK0kaHYNekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNa5X0CfZmuREkqkke2Y4virJ/d3xI0nGB459tGs/keRHFrF2SVIPcwZ9khXAPcDNwCbg1iSbhrrdBjxTVRuAu4G7utduAnYAbwC2Ar/d/T5J0mXSZ0a/BZiqqpNV9TxwENg21GcbcF+3fQi4MUm69oNV9bWq+hIw1f0+SdJlkqp66Q7JdmBrVf1Mt/+TwA1VtXugzxe7Pqe7/SeAG4A7gH+oqj/q2vcDn62qQ0N/xy5gV7f7fcCJhQ/tm64Dnl7E33e1WI7jXo5jhuU5bsf8rV5XVWMzHVg5mnouTVXtA/aN4ncnmayqzaP43Vey5Tju5ThmWJ7jdsyXps/SzRlg3cD+2q5txj5JVgLXAud6vlaSNEJ9gv4osDHJ+iTXMH1xdWKozwSws9veDjxc02tCE8CO7ls564GNwD8uTumSpD7mXLqpqotJdgMPASuAe6vqeJK9wGRVTQD7gQNJpoDzTL8Z0PV7AHgMuAh8qKq+MaKxzGYkS0JXgeU47uU4Zlie43bMl2DOi7GSpKubd8ZKUuMMeklqXNNBP9ejG1qU5FSSR5M8kmRyqesZlST3Jvlqdw/Hi23fleQvk/xr9+erl7LGxTbLmO9IcqY7348k+dGlrHGxJVmX5K+SPJbkeJKf69pbP9ezjXte57vZNfruUQv/AtwEnGb620O3VtVjS1rYiCU5BWyuqqZvJknyduA54JNV9cau7ePA+aq6s3tjf3VVfWQp61xMs4z5DuC5qvq1paxtVJK8FnhtVX0+yXcCx4B3Az9N2+d6tnG/l3mc75Zn9H0e3aCrVFX9DdPf8Bo0+CiO+5j+H6MZs4y5aVX1VFV9vtv+T+BxYA3tn+vZxj0vLQf9GuDJgf3TLOA/1FWkgL9Icqx7tMRy8pqqeqrb/nfgNUtZzGW0O8k/d0s7TS1hDOqeivtm4AjL6FwPjRvmcb5bDvrl6q1V9YNMP230Q93H/WWnu2GvzXXJ/+93gO8B3gQ8Bfz6klYzIkleAXwa+Pmq+o/BYy2f6xnGPa/z3XLQL8vHL1TVme7PrwKfYXk9LfQr3drmi2ucX13iekauqr5SVd+oqheA36fB853k25kOuz+uqj/tmps/1zONe77nu+Wg7/PohqYkeXl34YYkLwfeCXzxpV/VlMFHcewE/nwJa7ksXgy7znto7Hx3jzvfDzxeVZ8YONT0uZ5t3PM9381+6wag++rRb/B/j2741aWtaLSSvJ7pWTxMP97iU62OOcmfAO9g+tGtXwF+Gfgz4AHgeuDLwHurqpmLl7OM+R1Mf4wv4BTwgYG166tekrcCfws8CrzQNf8i0+vVLZ/r2cZ9K/M4300HvSSp7aUbSRIGvSQ1z6CXpMYZ9JLUOINekhpn0EtS4wx6SWrc/wKfRrJxlnsuFAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use PCA to reduce dimensions of X to n components\n",
    "n = 25\n",
    "pca = sklearn.decomposition.PCA(n_components=n)\n",
    "pca.fit(X_train)\n",
    "plt.bar(x=range(n),height=pca.explained_variance_ratio_)\n",
    "reduced_X_train = pca.transform(X_train)\n",
    "reduced_X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5703125\n",
      "0.625\n",
      "0.61328125\n",
      "0.54296875\n",
      "0.5703125\n",
      "0.58203125\n",
      "0.49609375\n"
     ]
    }
   ],
   "source": [
    "# Basic classifiers on reduced data\n",
    "lr = LogisticRegression()\n",
    "lr.fit(reduced_X_train, Y_train)\n",
    "print(lr.score(reduced_X_test, Y_test))\n",
    "\n",
    "lrl1 = LogisticRegressionCV(cv=5,penalty=\"l1\",solver=\"liblinear\",\n",
    "                            max_iter=1000).fit(X_train, Y_train)\n",
    "lrl1.fit(X_train, Y_train)\n",
    "print(lrl1.score(X_test, Y_test))\n",
    "\n",
    "lrl2 = LogisticRegressionCV(cv=5,penalty=\"l2\",solver=\"liblinear\",\n",
    "                            max_iter=1000).fit(X_train, Y_train)\n",
    "lrl2.fit(X_train, Y_train)\n",
    "print(lrl2.score(X_test, Y_test))\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators=200)\n",
    "ada.fit(reduced_X_train, Y_train)\n",
    "print(ada.score(reduced_X_test, Y_test))\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400)\n",
    "rf.fit(reduced_X_train, Y_train)\n",
    "print(rf.score(reduced_X_test, Y_test))\n",
    "\n",
    "mlp = MLPClassifier(max_iter=10000)\n",
    "mlp.fit(reduced_X_train, Y_train)\n",
    "print(mlp.score(reduced_X_test, Y_test))\n",
    "\n",
    "gnb = GaussianNB().fit(reduced_X_train, Y_train)\n",
    "gnb.fit(reduced_X_train, Y_train)\n",
    "print(gnb.score(reduced_X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "8/8 [==============================] - 1s 5ms/step - loss: 1.2095 - accuracy: 0.5001\n",
      "Epoch 2/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.8691 - accuracy: 0.6342\n",
      "Epoch 3/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7765 - accuracy: 0.7621\n",
      "Epoch 4/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7479 - accuracy: 0.7589\n",
      "Epoch 5/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.7040 - accuracy: 0.8295\n",
      "Epoch 6/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6627 - accuracy: 0.8046\n",
      "Epoch 7/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6358 - accuracy: 0.8294\n",
      "Epoch 8/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.6181 - accuracy: 0.8656\n",
      "Epoch 9/10\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.5897 - accuracy: 0.8455\n",
      "Epoch 10/10\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.5585 - accuracy: 0.8593\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.1532 - accuracy: 0.5859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1532106399536133, 0.5859375]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg=1e-5\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='tanh', kernel_regularizer=L1L2(reg), input_dim=reduced_X_train.shape[1]))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(512, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(256, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(128, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(64, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(reduced_X_train, Y_train, epochs=10, verbose=True)\n",
    "model.evaluate(reduced_X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
