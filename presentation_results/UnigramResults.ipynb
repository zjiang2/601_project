{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import pickle\n",
    "import nltk\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.regularizers import l1,l2, L1L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 273958\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in tweets\n",
    "with open('../data/pickled_tweets/home_2019_3daysback_nolikecriterion.pkl', 'rb') as f:\n",
    "    home_2019_tweets = pickle.load(f)\n",
    "with open('../data/pickled_tweets/away_2019_3daysback_nolikecriterion.pkl', 'rb') as f:\n",
    "    away_2019_tweets = pickle.load(f)\n",
    "with open('../data/pickled_tweets/home_2020_3daysback_nolikecriterion.pkl', 'rb') as f:\n",
    "    home_2020_tweets = pickle.load(f)\n",
    "with open('../data/pickled_tweets/away_2020_3daysback_nolikecriterion.pkl', 'rb') as f:\n",
    "    away_2020_tweets = pickle.load(f)\n",
    "\n",
    "# load in nfl data\n",
    "s2020 = pd.read_csv('../data/season_data/2020_all_data.csv', index_col=0)\n",
    "s2019 = pd.read_csv('../data/season_data/2019_all_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a list of stopwords to remove.\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# lemmatize function\n",
    "def lemmatize(sentence, include_stopwords=False):\n",
    "    if include_stopwords:\n",
    "        return [WordNetLemmatizer().lemmatize(word) for word in sentence]\n",
    "    return [WordNetLemmatizer().lemmatize(word) for word in sentence if word not in stopwords]\n",
    "\n",
    "# preprocess the tweets - remove punctuation and lemmatize\n",
    "def preprocess(tweets):\n",
    "    for i in range(len(tweets)):\n",
    "        for j in range(len(tweets[i])):\n",
    "            tweets[i][j] = re.sub('[^a-zA-Z]',' ',tweets[i][j]).split()\n",
    "            tweets[i][j] = lemmatize(tweets[i][j])\n",
    "\n",
    "preprocess(home_2019_tweets)\n",
    "preprocess(away_2019_tweets)\n",
    "preprocess(home_2020_tweets)\n",
    "preprocess(away_2020_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def vectorize_list(list_of_tweets, corpus):\n",
    "    num_tweets = len(list_of_tweets)\n",
    "    with_repeats = [item for sublist in list_of_tweets for item in sublist]\n",
    "    counts = dict(Counter(with_repeats))\n",
    "    to_return = []\n",
    "    for key in corpus:\n",
    "        num = np.log(1+counts[key]) if key in counts else 0\n",
    "        to_return.append(num)\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count unigrams\n",
    "def count_unigrams(tweet,corpus):\n",
    "    for word in tweet:\n",
    "        if word in corpus:\n",
    "            corpus[word] += 1\n",
    "        else:\n",
    "            corpus[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the home/away corpus\n",
    "home_corpus = {}\n",
    "away_corpus = {}\n",
    "for tweets in home_2019_tweets:\n",
    "    for tw in tweets:\n",
    "        count_unigrams(tw,home_corpus)\n",
    "\n",
    "for tweets in away_2019_tweets:\n",
    "    for tw in tweets:\n",
    "        count_unigrams(tw,away_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words with <= 2 characters\n",
    "for key in list(home_corpus.keys()):\n",
    "    if len(key) <= 2:\n",
    "        del home_corpus[key]\n",
    "\n",
    "for key in list(away_corpus.keys()):\n",
    "    if len(key) <= 2:\n",
    "        del away_corpus[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of tweets for all home and away teams\n",
    "n_home_tweets = sum([len(game_tweets) for game_tweets in home_2019_tweets])\n",
    "n_away_tweets = sum([len(game_tweets) for game_tweets in away_2019_tweets])\n",
    "\n",
    "# get the unigrams that appear in at least 0.1% of home/away tweets\n",
    "home_top_grams = [word for word in home_corpus if home_corpus[word] > n_home_tweets*0.001]\n",
    "away_top_grams = [word for word in away_corpus if away_corpus[word] > n_away_tweets*0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN SET\n",
    "#Create lists of vectors for home, away games\n",
    "num_h_tweets = [vectorize_list(game, home_top_grams) for game in home_2019_tweets]\n",
    "num_a_tweets = [vectorize_list(game, away_top_grams) for game in away_2019_tweets]\n",
    "\n",
    "#Turn into arrays\n",
    "home_vecs = np.array(num_h_tweets)\n",
    "away_vecs = np.array(num_a_tweets)\n",
    "\n",
    "#Concatenate home, away to form input matrix.\n",
    "X_train = np.concatenate([home_vecs, away_vecs], axis=1)\n",
    "\n",
    "# TEST SET\n",
    "#Create lists of vectors for home, away games\n",
    "num_h_tweets = [vectorize_list(game, home_top_grams) for game in home_2020_tweets]\n",
    "num_a_tweets = [vectorize_list(game, away_top_grams) for game in away_2020_tweets]\n",
    "\n",
    "#Turn into arrays\n",
    "home_vecs = np.array(num_h_tweets)\n",
    "away_vecs = np.array(num_a_tweets)\n",
    "\n",
    "#Concatenate home, away to form input matrix.\n",
    "X_test = np.concatenate([home_vecs, away_vecs], axis=1)\n",
    "\n",
    "# TRAIN and TEST outcomes\n",
    "Y_train = np.array(s2019[\"Home Win\"])\n",
    "Y_test = np.array(s2020[\"Home Win\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.578125\n",
      "0.58203125\n",
      "0.625\n",
      "0.51953125\n",
      "0.55859375\n",
      "0.5703125\n"
     ]
    }
   ],
   "source": [
    "# Basic classifiers on full data\n",
    "lr = LogisticRegression(penalty=\"none\")\n",
    "lr.fit(X_train, Y_train)\n",
    "print(lr.score(X_test, Y_test))\n",
    "\n",
    "lrl1 = LogisticRegressionCV(cv=5,penalty=\"l1\",solver=\"liblinear\",\n",
    "                            max_iter=1000).fit(X_train, Y_train)\n",
    "lrl1.fit(X_train, Y_train)\n",
    "print(lrl1.score(X_test, Y_test))\n",
    "\n",
    "lrl2 = LogisticRegressionCV(cv=5,penalty=\"l2\",solver=\"liblinear\",\n",
    "                            max_iter=1000).fit(X_train, Y_train)\n",
    "lrl2.fit(X_train, Y_train)\n",
    "print(lrl2.score(X_test, Y_test))\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators=200)\n",
    "ada.fit(X_train, Y_train)\n",
    "print(ada.score(X_test, Y_test))\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400)\n",
    "rf.fit(X_train, Y_train)\n",
    "print(rf.score(X_test, Y_test))\n",
    "\n",
    "gnb = GaussianNB().fit(X_train, Y_train)\n",
    "gnb.fit(X_train, Y_train)\n",
    "print(gnb.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 2.2759 - accuracy: 0.4258\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9886 - accuracy: 0.4763\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.9141 - accuracy: 0.4894\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8531 - accuracy: 0.5539\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8679 - accuracy: 0.5088\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8680 - accuracy: 0.4990\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8562 - accuracy: 0.5489\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8539 - accuracy: 0.5325\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8538 - accuracy: 0.6232\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8468 - accuracy: 0.5291\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8436 - accuracy: 0.5236\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8348 - accuracy: 0.6078\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8255 - accuracy: 0.5306\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8229 - accuracy: 0.6961\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.8079 - accuracy: 0.7662\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7780 - accuracy: 0.8071\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7510 - accuracy: 0.8073\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7333 - accuracy: 0.8563\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.7101 - accuracy: 0.8475\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6877 - accuracy: 0.8711\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.8178 - accuracy: 0.5781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8177796602249146, 0.578125]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg=1e-5\n",
    "model = Sequential()\n",
    "#model.add(Dense(1024, activation='tanh', kernel_regularizer=L1L2(reg), input_dim=X_train.shape[1]))\n",
    "#model.add(Dropout(.1))\n",
    "#model.add(Dense(512, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(256, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "#model.add(Dense(128, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(64, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=20, verbose=True)\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use PCA to reduce dimensions of X to n components\n",
    "n = 10\n",
    "pca = sklearn.decomposition.PCA(n_components=n)\n",
    "pca.fit(X_train)\n",
    "plt.bar(x=range(n),height=pca.explained_variance_ratio_)\n",
    "reduced_X_train = pca.transform(X_train)\n",
    "reduced_X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.60546875\n",
      "0.58984375\n",
      "0.609375\n",
      "0.546875\n",
      "0.61328125\n",
      "0.52734375\n"
     ]
    }
   ],
   "source": [
    "# Basic classifiers on reduced data\n",
    "lr = LogisticRegression()\n",
    "lr.fit(reduced_X_train, Y_train)\n",
    "print(lr.score(reduced_X_test, Y_test))\n",
    "\n",
    "lrl1 = LogisticRegressionCV(cv=5,penalty=\"l1\",solver=\"liblinear\",\n",
    "                            max_iter=1000).fit(reduced_X_train, Y_train)\n",
    "lrl1.fit(reduced_X_train, Y_train)\n",
    "print(lrl1.score(reduced_X_test, Y_test))\n",
    "\n",
    "lrl2 = LogisticRegressionCV(cv=5,penalty=\"l2\",solver=\"liblinear\",\n",
    "                            max_iter=1000).fit(reduced_X_train, Y_train)\n",
    "lrl2.fit(reduced_X_train, Y_train)\n",
    "print(lrl2.score(reduced_X_test, Y_test))\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators=200)\n",
    "ada.fit(reduced_X_train, Y_train)\n",
    "print(ada.score(reduced_X_test, Y_test))\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400)\n",
    "rf.fit(reduced_X_train, Y_train)\n",
    "print(rf.score(reduced_X_test, Y_test))\n",
    "\n",
    "gnb = GaussianNB().fit(reduced_X_train, Y_train)\n",
    "gnb.fit(reduced_X_train, Y_train)\n",
    "print(gnb.score(reduced_X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.7532 - accuracy: 0.5846\n",
      "Epoch 2/20\n",
      "8/8 [==============================] - 0s 879us/step - loss: 0.6389 - accuracy: 0.6085\n",
      "Epoch 3/20\n",
      "8/8 [==============================] - 0s 859us/step - loss: 0.6121 - accuracy: 0.6889\n",
      "Epoch 4/20\n",
      "8/8 [==============================] - 0s 854us/step - loss: 0.6029 - accuracy: 0.6467\n",
      "Epoch 5/20\n",
      "8/8 [==============================] - 0s 863us/step - loss: 0.5531 - accuracy: 0.7268\n",
      "Epoch 6/20\n",
      "8/8 [==============================] - 0s 866us/step - loss: 0.5138 - accuracy: 0.7649\n",
      "Epoch 7/20\n",
      "8/8 [==============================] - 0s 810us/step - loss: 0.5136 - accuracy: 0.7888\n",
      "Epoch 8/20\n",
      "8/8 [==============================] - 0s 878us/step - loss: 0.4881 - accuracy: 0.8176\n",
      "Epoch 9/20\n",
      "8/8 [==============================] - 0s 834us/step - loss: 0.4439 - accuracy: 0.8254\n",
      "Epoch 10/20\n",
      "8/8 [==============================] - 0s 832us/step - loss: 0.4454 - accuracy: 0.8393\n",
      "Epoch 11/20\n",
      "8/8 [==============================] - 0s 819us/step - loss: 0.4247 - accuracy: 0.8725\n",
      "Epoch 12/20\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.4040 - accuracy: 0.8853\n",
      "Epoch 13/20\n",
      "8/8 [==============================] - 0s 836us/step - loss: 0.3896 - accuracy: 0.8611\n",
      "Epoch 14/20\n",
      "8/8 [==============================] - 0s 853us/step - loss: 0.3769 - accuracy: 0.9137\n",
      "Epoch 15/20\n",
      "8/8 [==============================] - 0s 888us/step - loss: 0.3568 - accuracy: 0.9334\n",
      "Epoch 16/20\n",
      "8/8 [==============================] - 0s 909us/step - loss: 0.3426 - accuracy: 0.8885\n",
      "Epoch 17/20\n",
      "8/8 [==============================] - 0s 851us/step - loss: 0.3284 - accuracy: 0.9116\n",
      "Epoch 18/20\n",
      "8/8 [==============================] - 0s 880us/step - loss: 0.3239 - accuracy: 0.9135\n",
      "Epoch 19/20\n",
      "8/8 [==============================] - 0s 851us/step - loss: 0.2873 - accuracy: 0.9292\n",
      "Epoch 20/20\n",
      "8/8 [==============================] - 0s 826us/step - loss: 0.2984 - accuracy: 0.9490\n",
      "8/8 [==============================] - 0s 813us/step - loss: 0.7699 - accuracy: 0.5469\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7698532938957214, 0.546875]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg=1e-5\n",
    "model = Sequential()\n",
    "#model.add(Dense(1024, activation='tanh', kernel_regularizer=L1L2(reg), input_dim=reduced_X_train.shape[1]))\n",
    "#model.add(Dropout(.1))\n",
    "#model.add(Dense(512, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(256, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "#model.add(Dense(128, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(64, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(reduced_X_train, Y_train, epochs=20, verbose=True)\n",
    "model.evaluate(reduced_X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
