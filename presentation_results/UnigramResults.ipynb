{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import pickle\n",
    "import nltk\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import *\n",
    "from keras.regularizers import l1,l2, L1L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 273958\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in tweets\n",
    "with open('../data/pickled_tweets/home_2019_3daysback_nolikecriterion.pkl', 'rb') as f:\n",
    "    home_2019_tweets = pickle.load(f)\n",
    "with open('../data/pickled_tweets/away_2019_3daysback_nolikecriterion.pkl', 'rb') as f:\n",
    "    away_2019_tweets = pickle.load(f)\n",
    "with open('../data/pickled_tweets/home_2020_3daysback_nolikecriterion.pkl', 'rb') as f:\n",
    "    home_2020_tweets = pickle.load(f)\n",
    "with open('../data/pickled_tweets/away_2020_3daysback_nolikecriterion.pkl', 'rb') as f:\n",
    "    away_2020_tweets = pickle.load(f)\n",
    "\n",
    "# load in nfl data\n",
    "s2020 = pd.read_csv('../data/season_data/2020_all_data.csv', index_col=0)\n",
    "s2019 = pd.read_csv('../data/season_data/2019_all_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a list of stopwords to remove.\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# lemmatize function\n",
    "def lemmatize(sentence, include_stopwords=False):\n",
    "    if include_stopwords:\n",
    "        return [WordNetLemmatizer().lemmatize(word) for word in sentence]\n",
    "    return [WordNetLemmatizer().lemmatize(word) for word in sentence if word not in stopwords]\n",
    "\n",
    "# preprocess the tweets - remove punctuation and lemmatize\n",
    "def preprocess(tweets):\n",
    "    for i in range(len(tweets)):\n",
    "        for j in range(len(tweets[i])):\n",
    "            tweets[i][j] = re.sub('[^a-zA-Z]',' ',tweets[i][j]).split()\n",
    "            tweets[i][j] = lemmatize(tweets[i][j])\n",
    "\n",
    "preprocess(home_2019_tweets)\n",
    "preprocess(away_2019_tweets)\n",
    "preprocess(home_2020_tweets)\n",
    "preprocess(away_2020_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def vectorize_list(list_of_tweets, corpus):\n",
    "    num_tweets = len(list_of_tweets)\n",
    "    with_repeats = [item for sublist in list_of_tweets for item in sublist]\n",
    "    counts = dict(Counter(with_repeats))\n",
    "    to_return = []\n",
    "    for key in corpus:\n",
    "        num = np.log(1+counts[key]) if key in counts else 0\n",
    "        to_return.append(num)\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count unigrams\n",
    "def count_unigrams(tweet,corpus):\n",
    "    for word in tweet:\n",
    "        if word in corpus:\n",
    "            corpus[word] += 1\n",
    "        else:\n",
    "            corpus[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the home/away corpus\n",
    "home_corpus = {}\n",
    "away_corpus = {}\n",
    "for tweets in home_2019_tweets:\n",
    "    for tw in tweets:\n",
    "        count_unigrams(tw,home_corpus)\n",
    "\n",
    "for tweets in away_2019_tweets:\n",
    "    for tw in tweets:\n",
    "        count_unigrams(tw,away_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words with <= 2 characters\n",
    "for key in list(home_corpus.keys()):\n",
    "    if len(key) <= 2:\n",
    "        del home_corpus[key]\n",
    "\n",
    "for key in list(away_corpus.keys()):\n",
    "    if len(key) <= 2:\n",
    "        del away_corpus[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of tweets for all home and away teams\n",
    "n_home_tweets = sum([len(game_tweets) for game_tweets in home_2019_tweets])\n",
    "n_away_tweets = sum([len(game_tweets) for game_tweets in away_2019_tweets])\n",
    "\n",
    "# get the unigrams that appear in at least 0.1% of home/away tweets\n",
    "home_top_grams = [word for word in home_corpus if home_corpus[word] > n_home_tweets*0.001]\n",
    "away_top_grams = [word for word in away_corpus if away_corpus[word] > n_away_tweets*0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN SET\n",
    "#Create lists of vectors for home, away games\n",
    "num_h_tweets = [vectorize_list(game, home_top_grams) for game in home_2019_tweets]\n",
    "num_a_tweets = [vectorize_list(game, away_top_grams) for game in away_2019_tweets]\n",
    "\n",
    "#Turn into arrays\n",
    "home_vecs = np.array(num_h_tweets)\n",
    "away_vecs = np.array(num_a_tweets)\n",
    "\n",
    "#Concatenate home, away to form input matrix.\n",
    "X_train = np.concatenate([home_vecs, away_vecs], axis=1)\n",
    "\n",
    "# TEST SET\n",
    "#Create lists of vectors for home, away games\n",
    "num_h_tweets = [vectorize_list(game, home_top_grams) for game in home_2020_tweets]\n",
    "num_a_tweets = [vectorize_list(game, away_top_grams) for game in away_2020_tweets]\n",
    "\n",
    "#Turn into arrays\n",
    "home_vecs = np.array(num_h_tweets)\n",
    "away_vecs = np.array(num_a_tweets)\n",
    "\n",
    "#Concatenate home, away to form input matrix.\n",
    "X_test = np.concatenate([home_vecs, away_vecs], axis=1)\n",
    "\n",
    "# TRAIN and TEST outcomes\n",
    "Y_train = np.array(full[\"Home Win\"])[512:768]\n",
    "Y_test = np.array(full[\"Home Win\"])[768:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.578125\n",
      "0.58203125\n",
      "0.625\n",
      "0.51953125\n",
      "0.55859375\n",
      "0.5703125\n"
     ]
    }
   ],
   "source": [
    "# Basic classifiers on full data\n",
    "lr = LogisticRegression(penalty=\"none\")\n",
    "lr.fit(X_train, Y_train)\n",
    "print(lr.score(X_test, Y_test))\n",
    "\n",
    "lrl1 = LogisticRegressionCV(cv=5,penalty=\"l1\",solver=\"liblinear\",\n",
    "                            max_iter=1000).fit(X_train, Y_train)\n",
    "lrl1.fit(X_train, Y_train)\n",
    "print(lrl1.score(X_test, Y_test))\n",
    "\n",
    "lrl2 = LogisticRegressionCV(cv=5,penalty=\"l2\",solver=\"liblinear\",\n",
    "                            max_iter=1000).fit(X_train, Y_train)\n",
    "lrl2.fit(X_train, Y_train)\n",
    "print(lrl2.score(X_test, Y_test))\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators=200)\n",
    "ada.fit(X_train, Y_train)\n",
    "print(ada.score(X_test, Y_test))\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400)\n",
    "rf.fit(X_train, Y_train)\n",
    "print(rf.score(X_test, Y_test))\n",
    "\n",
    "gnb = GaussianNB().fit(X_train, Y_train)\n",
    "gnb.fit(X_train, Y_train)\n",
    "print(gnb.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 1s 22ms/step - loss: 2.3581 - accuracy: 0.5344\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 1.4555 - accuracy: 0.5087\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 1.3758 - accuracy: 0.5106\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1.3703 - accuracy: 0.4451\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 1.3412 - accuracy: 0.5088\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 1.3252 - accuracy: 0.4990\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 19ms/step - loss: 1.3066 - accuracy: 0.5489\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1.2909 - accuracy: 0.5316\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 1.2777 - accuracy: 0.5311\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1.2626 - accuracy: 0.5291\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 1.2492 - accuracy: 0.5078\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 1.2334 - accuracy: 0.6372\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1.2180 - accuracy: 0.5874\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 1.2116 - accuracy: 0.5062\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1.1918 - accuracy: 0.5411\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 1.1786 - accuracy: 0.6041\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1.1638 - accuracy: 0.5344\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 1.1452 - accuracy: 0.7020\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 1.1468 - accuracy: 0.5586\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 1.1382 - accuracy: 0.5657\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1.0918 - accuracy: 0.7126\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 1.0509 - accuracy: 0.8244\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.9928 - accuracy: 0.8221\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.9186 - accuracy: 0.8569\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.9208 - accuracy: 0.7629\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.7768 - accuracy: 0.9192\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.7222 - accuracy: 0.8824\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.7230 - accuracy: 0.8613\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.6789 - accuracy: 0.8809\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 18ms/step - loss: 0.6424 - accuracy: 0.8959\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.5623 - accuracy: 0.9347\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.6590 - accuracy: 0.8988\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.5066 - accuracy: 0.9746\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.4887 - accuracy: 0.9587\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.4602 - accuracy: 0.9623\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.5118 - accuracy: 0.9260\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.5248 - accuracy: 0.9350\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.4848 - accuracy: 0.9326\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.4470 - accuracy: 0.9488\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.4054 - accuracy: 0.9708\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.4134 - accuracy: 0.9500\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.3400 - accuracy: 0.9835\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 21ms/step - loss: 0.3239 - accuracy: 0.9963\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 20ms/step - loss: 0.3216 - accuracy: 0.9928\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.3064 - accuracy: 0.9965\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.2904 - accuracy: 0.9986\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 24ms/step - loss: 0.2826 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.2781 - accuracy: 0.9901\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 23ms/step - loss: 0.2782 - accuracy: 0.9907\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 22ms/step - loss: 0.2612 - accuracy: 0.9965\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 1.8445 - accuracy: 0.5430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.8445426225662231, 0.54296875]"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg=1e-5\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='tanh', kernel_regularizer=L1L2(reg), input_dim=X_train.shape[1]))\n",
    "#model.add(Dropout(.1))\n",
    "#model.add(Dense(512, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(256, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "#model.add(Dense(128, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(64, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=50, verbose=True)\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD5CAYAAAAp8/5SAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAASR0lEQVR4nO3df6xf933X8edr9pK1jZaU9Aox28Oe4oFu6di6G69jNKCGdQ6FGDRnOB0smSJ5aDMMWDVckLziDamB0XTSDKppsmXJihN5HbLI3byKIE2aSvBNWhJuPMOdG+LrFvU2STOyKXNv8+aP7zH67rvr3HNzf3ydz30+JMvnfD6fc877KM7rnPs53++5qSokSe36hnEXIElaXwa9JDXOoJekxhn0ktQ4g16SGmfQS1LjtvYZlGQv8AvAFuCTVfXRkf5bgI8D3wEcqKqTQ33fCnwS2AEU8Ner6rkrHesd73hH7dy5c0UnIUmb3ZNPPvmVqppYqm/ZoE+yBTgGfD8wD5xJcqqqnh0a9jxwN/ChJXbxK8C/rKrPJLkOeO31jrdz505mZmaWK0uSNCTJ/75SX587+j3AXFWd73Z2AtgH/P+gv3yHnuSPhXiSSWBrVX2mG/fKSouXJK1Onzn6bcCFofX5rq2Pbwe+muTTST6X5F93PyFIkjbIej+M3Qq8l8GUzs3AtzGY4vljkhxMMpNkZmFhYZ1LkqTNpU/QX2TwIPWy7V1bH/PA56vqfFUtAv8RePfooKo6XlVTVTU1MbHkswRJ0hvUJ+jPALuT7EpyDXAAONVz/2eAG5JcTu/3MTS3L0laf8sGfXcnfgg4DZwFHq2q2SRHk9wOkOTmJPPAHcAnksx2236dwbTNf07yDBDg36/PqUiSlpKr7TXFU1NT5ccrJWllkjxZVVNL9fnNWElqnEEvSY3r9QqEN5Odhx9b92M899EPrPsxJGmteEcvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS43oFfZK9Sc4lmUtyeIn+W5I8lWQxyf4l+r85yXySX1yLoiVJ/S0b9Em2AMeA24BJ4M4kkyPDngfuBj51hd38LPDbb7xMSdIb1eeOfg8wV1Xnq+oScALYNzygqp6rqqeB10Y3TvLdwJ8GfmsN6pUkrVCfoN8GXBhan+/alpXkG4B/A3xomXEHk8wkmVlYWOiza0lST+v9MPbHgemqmn+9QVV1vKqmqmpqYmJinUuSpM2lzy8HvwjsGFrf3rX18b3Ae5P8OHAdcE2SV6rqTzzQlSStjz5BfwbYnWQXg4A/AHywz86r6ocvLye5G5gy5CVpYy07dVNVi8Ah4DRwFni0qmaTHE1yO0CSm5PMA3cAn0gyu55FS5L663NHT1VNA9MjbUeGls8wmNJ5vX38MvDLK65QkrQqfjNWkhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjegV9kr1JziWZS/Infrl3kluSPJVkMcn+ofbvTPLZJLNJnk7yd9ayeEnS8pYN+iRbgGPAbcAkcGeSyZFhzwN3A58aaf9D4Eeq6p3AXuDjSW5YZc2SpBXo88vB9wBzVXUeIMkJYB/w7OUBVfVc1/fa8IZV9T+Hlr+Y5MvABPDV1RYuSeqnz9TNNuDC0Pp817YiSfYA1wC/t0TfwSQzSWYWFhZWumtJ0uvYkIexSf4M8BDwo1X12mh/VR2vqqmqmpqYmNiIkiRp0+gT9BeBHUPr27u2XpJ8M/AY8M+r6r+urDxJ0mr1CfozwO4ku5JcAxwATvXZeTf+14FfqaqTb7xMSdIbtWzQV9UicAg4DZwFHq2q2SRHk9wOkOTmJPPAHcAnksx2m/8QcAtwd5LPd3++cz1ORJK0tD6fuqGqpoHpkbYjQ8tnGEzpjG73MPDwKmuUJK2C34yVpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1Lher0BQPzsPP7bux3juox9Y92NIaot39JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNa5X0CfZm+Rckrkkh5fovyXJU0kWk+wf6bsryf/q/ty1VoVLkvpZNuiTbAGOAbcBk8CdSSZHhj0P3A18amTbPwX8DPA9wB7gZ5K8ffVlS5L66nNHvweYq6rzVXUJOAHsGx5QVc9V1dPAayPb/gDwmap6sapeAj4D7F2DuiVJPfUJ+m3AhaH1+a6tj17bJjmYZCbJzMLCQs9dS5L6uCoexlbV8aqaqqqpiYmJcZcjSU3pE/QXgR1D69u7tj5Ws60kaQ30CfozwO4ku5JcAxwATvXc/2ng/Une3j2EfX/XJknaIMsGfVUtAocYBPRZ4NGqmk1yNMntAEluTjIP3AF8Islst+2LwM8yuFicAY52bZKkDdLrffRVNQ1Mj7QdGVo+w2BaZqltHwAeWEWNkqRVuCoexkqS1o9BL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqXK+gT7I3ybkkc0kOL9F/bZJHuv4nkuzs2r8xyYNJnklyNsmH17h+SdIylg36JFuAY8BtwCRwZ5LJkWH3AC9V1U3AfcC9XfsdwLVV9S7gu4Efu3wRkCRtjD539HuAuao6X1WXgBPAvpEx+4AHu+WTwK1JAhTwtiRbgbcAl4DfX5PKJUm99An6bcCFofX5rm3JMVW1CLwM3Mgg9P8A+BLwPPDzVfXiKmuWJK3Aej+M3QN8HfgWYBfwU0m+bXRQkoNJZpLMLCwsrHNJkrS59An6i8COofXtXduSY7ppmuuBF4APAr9ZVV+rqi8DvwNMjR6gqo5X1VRVTU1MTKz8LCRJV9Qn6M8Au5PsSnINcAA4NTLmFHBXt7wfeLyqisF0zfsAkrwNeA/wu2tRuCSpn2WDvptzPwScBs4Cj1bVbJKjSW7vht0P3JhkDvgnwOWPYB4Drksyy+CC8UtV9fRan4Qk6cq29hlUVdPA9EjbkaHlVxl8lHJ0u1eWapckbRy/GStJjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXG9gj7J3iTnkswlObxE/7VJHun6n0iyc6jvO5J8NslskmeSfNMa1i9JWsayQZ9kC4Nf8n0bMAncmWRyZNg9wEtVdRNwH3Bvt+1W4GHg71fVO4G/CnxtzaqXJC2rzx39HmCuqs5X1SXgBLBvZMw+4MFu+SRwa5IA7weerqr/DlBVL1TV19emdElSH32CfhtwYWh9vmtbckxVLQIvAzcC3w5UktNJnkry00sdIMnBJDNJZhYWFlZ6DpKk17HeD2O3An8Z+OHu77+d5NbRQVV1vKqmqmpqYmJinUuSpM2lT9BfBHYMrW/v2pYc083LXw+8wODu/7er6itV9YfANPDu1RYtSeqvT9CfAXYn2ZXkGuAAcGpkzCngrm55P/B4VRVwGnhXkrd2F4C/Ajy7NqVLkvrYutyAqlpMcohBaG8BHqiq2SRHgZmqOgXcDzyUZA54kcHFgKp6KcnHGFwsCpiuqsfW6VwkSUtYNugBqmqawbTLcNuRoeVXgTuusO3DDD5iKUkaA78ZK0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcb2CPsneJOeSzCU5vET/tUke6fqfSLJzpP9bk7yS5ENrVLckqadlgz7JFuAYcBswCdyZZHJk2D3AS1V1E3AfcO9I/8eA31h9uZKklepzR78HmKuq81V1CTgB7BsZsw94sFs+CdyaJABJ/hbwBWB2TSqWJK1In6DfBlwYWp/v2pYcU1WLwMvAjUmuA/4p8C9e7wBJDiaZSTKzsLDQt3ZJUg/r/TD2I8B9VfXK6w2qquNVNVVVUxMTE+tckiRtLlt7jLkI7Bha3961LTVmPslW4HrgBeB7gP1J/hVwA/Bakler6hdXW7gkqZ8+QX8G2J1kF4NAPwB8cGTMKeAu4LPAfuDxqirgvZcHJPkI8IohL0kba9mgr6rFJIeA08AW4IGqmk1yFJipqlPA/cBDSeaAFxlcDCRJV4E+d/RU1TQwPdJ2ZGj5VeCOZfbxkTdQnyRplfxmrCQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxvb4wpavfzsOPrfsxnvvoB9b9GJLWnnf0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUuF5Bn2RvknNJ5pIcXqL/2iSPdP1PJNnZtX9/kieTPNP9/b41rl+StIxlgz7JFuAYcBswCdyZZHJk2D3AS1V1E3AfcG/X/hXgb1bVu4C7gIfWqnBJUj997uj3AHNVdb6qLgEngH0jY/YBD3bLJ4Fbk6SqPldVX+zaZ4G3JLl2LQqXJPXTJ+i3AReG1ue7tiXHVNUi8DJw48iYHwSeqqo/Gj1AkoNJZpLMLCws9K1dktTDhjyMTfJOBtM5P7ZUf1Udr6qpqpqamJjYiJIkadPoE/QXgR1D69u7tiXHJNkKXA+80K1vB34d+JGq+r3VFixJWpk+76M/A+xOsotBoB8APjgy5hSDh62fBfYDj1dVJbkBeAw4XFW/s2ZV66riu/Clq9uyQV9Vi0kOAaeBLcADVTWb5CgwU1WngPuBh5LMAS8yuBgAHAJuAo4kOdK1vb+qvrzWJ6LNyYuMtLxev2GqqqaB6ZG2I0PLrwJ3LLHdzwE/t8oaJUmr4K8SlN4gf5rQm4WvQJCkxhn0ktQ4p26kNyGnjbQSBr2kFRnnRcYL3Btj0EtSD2/mi4xz9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuN6BX2SvUnOJZlLcniJ/muTPNL1P5Fk51Dfh7v2c0l+YA1rlyT1sGzQJ9kCHANuAyaBO5NMjgy7B3ipqm4C7gPu7badZPD7Y98J7AX+bbc/SdIG6XNHvweYq6rzVXUJOAHsGxmzD3iwWz4J3JokXfuJqvqjqvoCMNftT5K0QfoE/TbgwtD6fNe25JiqWgReBm7sua0kaR1dFe+jT3IQONitvpLk3AYe/h3AV1ayQe5dp0o29tie98Yfe8U87zWxonN/E5/3n71SR5+gvwjsGFrf3rUtNWY+yVbgeuCFnttSVceB4z1qWXNJZqpqahzHHifPe3PZrOcNm/vcL+szdXMG2J1kV5JrGDxcPTUy5hRwV7e8H3i8qqprP9B9KmcXsBv4b2tTuiSpj2Xv6KtqMckh4DSwBXigqmaTHAVmquoUcD/wUJI54EUGFwO6cY8CzwKLwE9U1dfX6VwkSUvI4MZ780pysJs62lQ8781ls543bO5zv2zTB70ktc5XIEhS4zZ10C/3aocWJdmR5L8keTbJbJKfHHdNGynJliSfS/Kfxl3LRklyQ5KTSX43ydkk3zvumjZCkn/c/Rv/H0n+Q5JvGndN47Jpg77nqx1atAj8VFVNAu8BfmKTnPdlPwmcHXcRG+wXgN+sqj8P/EU2wfkn2Qb8Q2Cqqv4Cgw+SHBhvVeOzaYOefq92aE5VfamqnuqW/y+D/+k3xbeVk2wHPgB8cty1bJQk1wO3MPhkHFV1qaq+OtaiNs5W4C3dd3veCnxxzPWMzWYO+k3/eobuLaPfBTwx5lI2yseBnwZeG3MdG2kXsAD8Ujdl9ckkbxt3Ueutqi4CPw88D3wJeLmqfmu8VY3PZg76TS3JdcCvAf+oqn5/3PWstyR/A/hyVT057lo22Fbg3cC/q6rvAv4AaP55VJK3M/gJfRfwLcDbkvzd8VY1Pps56Hu9nqFFSb6RQcj/alV9etz1bJDvA25P8hyDabr3JXl4vCVtiHlgvqou/9R2kkHwt+6vAV+oqoWq+hrwaeAvjbmmsdnMQd/n1Q7N6V4ffT9wtqo+Nu56NkpVfbiqtlfVTgb/rR+vqubv8Krq/wAXkvy5rulWBt9Ub93zwHuSvLX7N38rm+Ah9JVcFW+vHIcrvdphzGVthO8D/h7wTJLPd23/rKqmx1eS1tk/AH61u6E5D/zomOtZd1X1RJKTwFMMPmn2Ocb04sSrgd+MlaTGbeapG0naFAx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIa9/8AhQtFkbSfQOAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use PCA to reduce dimensions of X to n components\n",
    "n = 10\n",
    "pca = sklearn.decomposition.PCA(n_components=n)\n",
    "pca.fit(X_train)\n",
    "plt.bar(x=range(n),height=pca.explained_variance_ratio_)\n",
    "reduced_X_train = pca.transform(X_train)\n",
    "reduced_X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.60546875\n",
      "0.58984375\n",
      "0.609375\n",
      "0.546875\n",
      "0.61328125\n",
      "0.52734375\n"
     ]
    }
   ],
   "source": [
    "# Basic classifiers on reduced data\n",
    "lr = LogisticRegression()\n",
    "lr.fit(reduced_X_train, Y_train)\n",
    "print(lr.score(reduced_X_test, Y_test))\n",
    "\n",
    "lrl1 = LogisticRegressionCV(cv=5,penalty=\"l1\",solver=\"liblinear\",\n",
    "                            max_iter=1000).fit(reduced_X_train, Y_train)\n",
    "lrl1.fit(reduced_X_train, Y_train)\n",
    "print(lrl1.score(reduced_X_test, Y_test))\n",
    "\n",
    "lrl2 = LogisticRegressionCV(cv=5,penalty=\"l2\",solver=\"liblinear\",\n",
    "                            max_iter=1000).fit(reduced_X_train, Y_train)\n",
    "lrl2.fit(reduced_X_train, Y_train)\n",
    "print(lrl2.score(reduced_X_test, Y_test))\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators=200)\n",
    "ada.fit(reduced_X_train, Y_train)\n",
    "print(ada.score(reduced_X_test, Y_test))\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=400)\n",
    "rf.fit(reduced_X_train, Y_train)\n",
    "print(rf.score(reduced_X_test, Y_test))\n",
    "\n",
    "gnb = GaussianNB().fit(reduced_X_train, Y_train)\n",
    "gnb.fit(reduced_X_train, Y_train)\n",
    "print(gnb.score(reduced_X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2780 - accuracy: 0.5203\n",
      "Epoch 2/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7149 - accuracy: 0.7065\n",
      "Epoch 3/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.7194 - accuracy: 0.6518\n",
      "Epoch 4/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.6587 - accuracy: 0.7094\n",
      "Epoch 5/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.6103 - accuracy: 0.7879\n",
      "Epoch 6/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5606 - accuracy: 0.7863\n",
      "Epoch 7/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.5488 - accuracy: 0.8489\n",
      "Epoch 8/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.5240 - accuracy: 0.8819\n",
      "Epoch 9/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4840 - accuracy: 0.8587\n",
      "Epoch 10/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4793 - accuracy: 0.9052\n",
      "Epoch 11/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.4470 - accuracy: 0.8777\n",
      "Epoch 12/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4420 - accuracy: 0.9036\n",
      "Epoch 13/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.4161 - accuracy: 0.9197\n",
      "Epoch 14/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3946 - accuracy: 0.9438\n",
      "Epoch 15/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3767 - accuracy: 0.9374\n",
      "Epoch 16/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3516 - accuracy: 0.9448\n",
      "Epoch 17/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3415 - accuracy: 0.9512\n",
      "Epoch 18/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.3338 - accuracy: 0.9504\n",
      "Epoch 19/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3198 - accuracy: 0.9556\n",
      "Epoch 20/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.3230 - accuracy: 0.9641\n",
      "Epoch 21/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2816 - accuracy: 0.9804\n",
      "Epoch 22/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2882 - accuracy: 0.9868\n",
      "Epoch 23/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2645 - accuracy: 0.9840\n",
      "Epoch 24/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2452 - accuracy: 0.9927\n",
      "Epoch 25/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2453 - accuracy: 0.9885\n",
      "Epoch 26/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2508 - accuracy: 0.9753\n",
      "Epoch 27/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.2793 - accuracy: 0.9389\n",
      "Epoch 28/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2582 - accuracy: 0.9644\n",
      "Epoch 29/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2183 - accuracy: 0.9896\n",
      "Epoch 30/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1976 - accuracy: 0.9924\n",
      "Epoch 31/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.2064 - accuracy: 0.9864\n",
      "Epoch 32/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1840 - accuracy: 0.9967\n",
      "Epoch 33/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1970 - accuracy: 0.9939\n",
      "Epoch 34/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1862 - accuracy: 0.9889\n",
      "Epoch 35/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1671 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1534 - accuracy: 1.0000\n",
      "Epoch 37/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1485 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1460 - accuracy: 1.0000\n",
      "Epoch 39/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1406 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1374 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1344 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1344 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1285 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1264 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1241 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1234 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1218 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1202 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.1186 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.1162 - accuracy: 1.0000\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 1.4239 - accuracy: 0.5664\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4238886833190918, 0.56640625]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg=1e-5\n",
    "model = Sequential()\n",
    "model.add(Dense(1024, activation='tanh', kernel_regularizer=L1L2(reg), input_dim=reduced_X_train.shape[1]))\n",
    "#model.add(Dropout(.1))\n",
    "#model.add(Dense(512, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(256, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "#model.add(Dense(128, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(64, activation='tanh',kernel_regularizer=L1L2(reg)))\n",
    "#model.add(Dropout(.1))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(reduced_X_train, Y_train, epochs=50, verbose=True)\n",
    "model.evaluate(reduced_X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
