{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the hashtag list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_hashes = {}\n",
    "with open('teams_hashtags.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        words = line.split()\n",
    "        team = words[0].replace('-', ' ')\n",
    "        hashes = words[1:]\n",
    "        team_hashes[team] = hashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to get all hashtags from a given tweet; function below takes a tweet and a team name (that we want that tweet to only be associated with) and checks if it contains tweets from any other team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags(tweet):\n",
    "    pattern = r'#{1}\\w*'\n",
    "    return re.findall(pattern, tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_team_only(one_tweet, which_team):\n",
    "    all_teams = team_hashes.copy()\n",
    "    good = all_teams.pop(which_team)\n",
    "    temp = [team_hashes[key] for key in all_teams]\n",
    "    bad_teams = [item for sublist in temp for item in sublist]\n",
    "    bad_teams = set(bad_teams)\n",
    "    curr_hashes = set(get_hashtags(one_tweet))\n",
    "    \n",
    "    if len(list(curr_hashes & bad_teams)) != 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data for a full season, and convert the type of the datetime column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_season = pd.read_csv('2020_all_data.csv', index_col=0)\n",
    "full_season['Datetime'] = pd.to_datetime(full_season['Datetime'], utc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize containers. Home_tweets contains all tweets related to the home teams, same for away, and corpus contains all the unique words seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_tweets = []\n",
    "away_tweets = []\n",
    "corpus = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count unigrams\n",
    "def count_unigrams(tweet):\n",
    "    global corpus\n",
    "    words = tweet.split(\" \")\n",
    "    for word in words:\n",
    "        if word in corpus:\n",
    "            corpus[word] += 1\n",
    "        else:\n",
    "            corpus[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bigrams from the list of tweets\n",
    "def count_bigrams(tweet):\n",
    "    global corpus\n",
    "    bigrams = nltk.bigrams(tweet.split(\" \"))\n",
    "    for bg in bigrams:\n",
    "        if bg in corpus:\n",
    "            corpus[bg] += 1\n",
    "        else:\n",
    "            corpus[bg] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns all valid tweets for one game, for one team. A valid tweet is one which is between the desired start and end times; has at least 10 likes; and only contains hashtags related to one team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_game_one_team(game_time, team_name):\n",
    "    start = game_time - datetime.timedelta(days=1)\n",
    "    end = game_time - datetime.timedelta(hours=1)\n",
    "    date_range = \" since:\" + start.strftime('%Y-%m-%d') + \" until:\" + end.strftime('%Y-%m-%d')\n",
    "    \n",
    "    team_tags = \" OR \".join(team_hashes[team_name])\n",
    "    query = team_tags + date_range\n",
    "    \n",
    "    # Scrape the tweets for the date_range. Also have to filter based on the \n",
    "    # time stamp so as not to capture tweets during and after games.\n",
    "    to_return = []\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
    "        if i > 10000:\n",
    "            break\n",
    "        if not (start <= tweet.date and tweet.date <= end):\n",
    "            continue\n",
    "        if tweet.likeCount < 10:\n",
    "            continue\n",
    "        if not one_team_only(tweet.content.lower(), team_name):\n",
    "            continue\n",
    "        tw = tweet.content.lower()\n",
    "        to_return.append(tw)\n",
    "        count_bigrams(tw)\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets all tweets for an entire season. Uses the above helper function to get a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season_tweets(df):\n",
    "    for game in range(len(df)):\n",
    "        print(\"Processing game number {}\".format(game))\n",
    "        gametime = df.iloc[game]['Datetime']\n",
    "        hometeam = df.iloc[game]['Home']\n",
    "        awayteam = df.iloc[game]['Away']\n",
    "        \n",
    "        h_tweets = one_game_one_team(gametime, hometeam)\n",
    "        a_tweets = one_game_one_team(gametime, awayteam)\n",
    "        \n",
    "        home_tweets.append(h_tweets)\n",
    "        away_tweets.append(a_tweets)\n",
    "        \n",
    "    return     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing game number 0\n",
      "Processing game number 1\n",
      "Processing game number 2\n",
      "Processing game number 3\n",
      "Processing game number 4\n",
      "Processing game number 5\n",
      "Processing game number 6\n",
      "Processing game number 7\n",
      "Processing game number 8\n",
      "Processing game number 9\n",
      "Processing game number 10\n",
      "Processing game number 11\n",
      "Processing game number 12\n",
      "Processing game number 13\n",
      "Processing game number 14\n",
      "Processing game number 15\n",
      "Processing game number 16\n",
      "Processing game number 17\n",
      "Processing game number 18\n",
      "Processing game number 19\n",
      "Processing game number 20\n",
      "Processing game number 21\n",
      "Processing game number 22\n",
      "Processing game number 23\n",
      "Processing game number 24\n",
      "Processing game number 25\n",
      "Processing game number 26\n",
      "Processing game number 27\n",
      "Processing game number 28\n",
      "Processing game number 29\n",
      "Processing game number 30\n",
      "Processing game number 31\n",
      "Processing game number 32\n",
      "Processing game number 33\n",
      "Processing game number 34\n",
      "Processing game number 35\n",
      "Processing game number 36\n",
      "Processing game number 37\n",
      "Processing game number 38\n",
      "Processing game number 39\n",
      "Processing game number 40\n",
      "Processing game number 41\n",
      "Processing game number 42\n",
      "Processing game number 43\n",
      "Processing game number 44\n",
      "Processing game number 45\n",
      "Processing game number 46\n",
      "Processing game number 47\n",
      "Processing game number 48\n",
      "Processing game number 49\n",
      "Processing game number 50\n",
      "Processing game number 51\n",
      "Processing game number 52\n",
      "Processing game number 53\n",
      "Processing game number 54\n",
      "Processing game number 55\n",
      "Processing game number 56\n",
      "Processing game number 57\n",
      "Processing game number 58\n",
      "Processing game number 59\n",
      "Processing game number 60\n",
      "Processing game number 61\n",
      "Processing game number 62\n",
      "Processing game number 63\n",
      "Processing game number 64\n",
      "Processing game number 65\n",
      "Processing game number 66\n",
      "Processing game number 67\n",
      "Processing game number 68\n",
      "Processing game number 69\n",
      "Processing game number 70\n",
      "Processing game number 71\n",
      "Processing game number 72\n",
      "Processing game number 73\n",
      "Processing game number 74\n",
      "Processing game number 75\n",
      "Processing game number 76\n",
      "Processing game number 77\n",
      "Processing game number 78\n",
      "Processing game number 79\n",
      "Processing game number 80\n",
      "Processing game number 81\n",
      "Processing game number 82\n",
      "Processing game number 83\n",
      "Processing game number 84\n",
      "Processing game number 85\n",
      "Processing game number 86\n",
      "Processing game number 87\n",
      "Processing game number 88\n",
      "Processing game number 89\n",
      "Processing game number 90\n",
      "Processing game number 91\n",
      "Processing game number 92\n",
      "Processing game number 93\n",
      "Processing game number 94\n",
      "Processing game number 95\n",
      "Processing game number 96\n",
      "Processing game number 97\n",
      "Processing game number 98\n",
      "Processing game number 99\n",
      "Processing game number 100\n",
      "Processing game number 101\n",
      "Processing game number 102\n",
      "Processing game number 103\n",
      "Processing game number 104\n",
      "Processing game number 105\n",
      "Processing game number 106\n",
      "Processing game number 107\n",
      "Processing game number 108\n",
      "Processing game number 109\n",
      "Processing game number 110\n",
      "Processing game number 111\n",
      "Processing game number 112\n",
      "Processing game number 113\n",
      "Processing game number 114\n",
      "Processing game number 115\n",
      "Processing game number 116\n",
      "Processing game number 117\n",
      "Processing game number 118\n",
      "Processing game number 119\n",
      "Processing game number 120\n",
      "Processing game number 121\n",
      "Processing game number 122\n",
      "Processing game number 123\n",
      "Processing game number 124\n",
      "Processing game number 125\n",
      "Processing game number 126\n",
      "Processing game number 127\n",
      "Processing game number 128\n",
      "Processing game number 129\n",
      "Processing game number 130\n",
      "Processing game number 131\n",
      "Processing game number 132\n",
      "Processing game number 133\n",
      "Processing game number 134\n",
      "Processing game number 135\n",
      "Processing game number 136\n",
      "Processing game number 137\n",
      "Processing game number 138\n",
      "Processing game number 139\n",
      "Processing game number 140\n",
      "Processing game number 141\n",
      "Processing game number 142\n",
      "Processing game number 143\n",
      "Processing game number 144\n",
      "Processing game number 145\n",
      "Processing game number 146\n",
      "Processing game number 147\n",
      "Processing game number 148\n",
      "Processing game number 149\n",
      "Processing game number 150\n",
      "Processing game number 151\n",
      "Processing game number 152\n",
      "Processing game number 153\n",
      "Processing game number 154\n",
      "Processing game number 155\n",
      "Processing game number 156\n",
      "Processing game number 157\n",
      "Processing game number 158\n",
      "Processing game number 159\n",
      "Processing game number 160\n",
      "Processing game number 161\n",
      "Processing game number 162\n",
      "Processing game number 163\n",
      "Processing game number 164\n",
      "Processing game number 165\n",
      "Processing game number 166\n",
      "Processing game number 167\n",
      "Processing game number 168\n",
      "Processing game number 169\n",
      "Processing game number 170\n",
      "Processing game number 171\n",
      "Processing game number 172\n",
      "Processing game number 173\n",
      "Processing game number 174\n",
      "Processing game number 175\n",
      "Processing game number 176\n",
      "Processing game number 177\n",
      "Processing game number 178\n",
      "Processing game number 179\n",
      "Processing game number 180\n",
      "Processing game number 181\n",
      "Processing game number 182\n",
      "Processing game number 183\n",
      "Processing game number 184\n",
      "Processing game number 185\n",
      "Processing game number 186\n",
      "Processing game number 187\n",
      "Processing game number 188\n",
      "Processing game number 189\n",
      "Processing game number 190\n",
      "Processing game number 191\n",
      "Processing game number 192\n",
      "Processing game number 193\n",
      "Processing game number 194\n",
      "Processing game number 195\n",
      "Processing game number 196\n",
      "Processing game number 197\n",
      "Processing game number 198\n",
      "Processing game number 199\n",
      "Processing game number 200\n",
      "Processing game number 201\n",
      "Processing game number 202\n",
      "Processing game number 203\n",
      "Processing game number 204\n",
      "Processing game number 205\n",
      "Processing game number 206\n",
      "Processing game number 207\n",
      "Processing game number 208\n",
      "Processing game number 209\n",
      "Processing game number 210\n",
      "Processing game number 211\n",
      "Processing game number 212\n",
      "Processing game number 213\n",
      "Processing game number 214\n",
      "Processing game number 215\n",
      "Processing game number 216\n",
      "Processing game number 217\n",
      "Processing game number 218\n",
      "Processing game number 219\n",
      "Processing game number 220\n",
      "Processing game number 221\n",
      "Processing game number 222\n",
      "Processing game number 223\n",
      "Processing game number 224\n",
      "Processing game number 225\n",
      "Processing game number 226\n",
      "Processing game number 227\n",
      "Processing game number 228\n",
      "Processing game number 229\n",
      "Processing game number 230\n",
      "Processing game number 231\n",
      "Processing game number 232\n",
      "Processing game number 233\n",
      "Processing game number 234\n",
      "Processing game number 235\n",
      "Processing game number 236\n",
      "Processing game number 237\n",
      "Processing game number 238\n",
      "Processing game number 239\n",
      "Processing game number 240\n",
      "Processing game number 241\n",
      "Processing game number 242\n",
      "Processing game number 243\n",
      "Processing game number 244\n",
      "Processing game number 245\n",
      "Processing game number 246\n",
      "Processing game number 247\n",
      "Processing game number 248\n",
      "Processing game number 249\n",
      "Processing game number 250\n",
      "Processing game number 251\n",
      "Processing game number 252\n",
      "Processing game number 253\n",
      "Processing game number 254\n",
      "Processing game number 255\n"
     ]
    }
   ],
   "source": [
    "get_season_tweets(full_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/pickled_tweets_corpus/2020season_home_bigrams.pkl', 'wb') as f:\n",
    "    pickle.dump(home_tweets, f)\n",
    "with open('data/pickled_tweets_corpus/2020season_away_bigrams.pkl', 'wb') as f:\n",
    "    pickle.dump(away_tweets, f)\n",
    "with open('data/pickled_tweets_corpus/2020season_corpus_bigrams.pkl', 'wb') as f:\n",
    "    pickle.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the top 100 or top 1000 unigrams or bigrams by frequency. Call this our corpus (can be changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_grams = [x[0] for x in sorted(corpus.items(), key=lambda x: x[1], reverse=True)[:3000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that, given a list of (valid) tweets for one game and a corpus of interest, returns a numerical version representation of the game. Each number in the return vector represents one word in the corpus, and it is computed as the number of occurrences of that word in tweets about this game, divided by the total number of tweets about this game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_list_bigrams(list_of_tweets, corpus):\n",
    "    num_tweets = len(list_of_tweets)\n",
    "    this_dict = {}\n",
    "    for tweet in list_of_tweets:\n",
    "        bigrams = nltk.bigrams(tweet.split(\" \"))\n",
    "        for bg in bigrams:\n",
    "            if bg in this_dict:\n",
    "                this_dict[bg] += 1\n",
    "            else:\n",
    "                this_dict[bg] = 1\n",
    "    to_return = []\n",
    "    for key in corpus:\n",
    "        num = this_dict[key]/num_tweets if key in this_dict else 0\n",
    "        to_return.append(num)\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.16666666666666666, 0.08333333333333333]"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_list_bigrams(home_tweets[0], [('to', 'the'), ('in', 'the')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize each home and each away game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_h_tweets = [vectorize_list_bigrams(game, top_grams) for game in home_tweets]\n",
    "num_a_tweets = [vectorize_list_bigrams(game, top_grams) for game in away_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the home and away vectors as in the papers, and use this to produce our X and Y to perform models on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_vecs = np.array(num_h_tweets)\n",
    "away_vecs = np.array(num_a_tweets)\n",
    "X = np.concatenate([home_vecs, away_vecs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = full_season.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 6000)"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/preprocessed/X_2020_1000bigrams', X)\n",
    "np.save('data/preprocessed/Y_2020_1000bigrams', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "everything = np.concatenate([X,Y.reshape(-1,1)], axis=1)\n",
    "np.random.shuffle(everything)\n",
    "xs = everything[:,:-1]\n",
    "ys = everything[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = xs[:200]\n",
    "X_test = xs[200:]\n",
    "Y_train = ys[:200]\n",
    "Y_test = ys[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6607142857142857"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, Y_train)\n",
    "lr.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5178571428571429"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier(n_estimators=100)\n",
    "ada.fit(X_train, Y_train)\n",
    "ada.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6071428571428571"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier().fit(X_train, Y_train)\n",
    "dt.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6071428571428571"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=1000)\n",
    "rf.fit(X_train, Y_train)\n",
    "rf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "mlp.fit(X_train, Y_train)\n",
    "mlp.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
