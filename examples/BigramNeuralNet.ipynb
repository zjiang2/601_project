{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import pickle\n",
    "import nltk\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer\n",
    "from keras.regularizers import l1,l2, L1L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 123\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in tweets\n",
    "with open('data/pickled_tweets/home_2020_3daysback_nolikecriterion.pkl', 'rb') as f:\n",
    "    home_tweets = pickle.load(f)\n",
    "with open('data/pickled_tweets/away_2020_3daysback_nolikecriterion.pkl', 'rb') as f:\n",
    "    away_tweets = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "season = pd.read_csv('data/season_data/2020_all_data.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bigrams from the list of tweets\n",
    "def count_bigrams(tweet, corpus):\n",
    "    words = tweet.split(\" \")\n",
    "    #words = [stemmer.stem(word) for word in words]\n",
    "    bigrams = nltk.bigrams(words)\n",
    "    for bg in bigrams:\n",
    "        if bg in corpus:\n",
    "            corpus[bg] += 1\n",
    "        else:\n",
    "            corpus[bg] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = int(np.floor(0.75*len(home_tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the home/away corpus\n",
    "home_corpus = {}\n",
    "away_corpus = {}\n",
    "for tweets in home_tweets[:train_test_split]:\n",
    "    for tw in tweets:\n",
    "        count_bigrams(tw,home_corpus)\n",
    "\n",
    "for tweets in away_tweets[:train_test_split]:\n",
    "    for tw in tweets:\n",
    "        count_bigrams(tw,away_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfor key in list(home_corpus.keys()):\\n    if len(key[0]) <= 2 or len(key[1]) <= 2:\\n        del home_corpus[key]\\n    elif key[0] in stopwords or key[1] in stopwords:\\n        del home_corpus[key]\\n    elif key[0][0] == '#' or key[1][0] == '#':\\n        del home_corpus[key]\\n\\nfor key in list(away_corpus.keys()):\\n    if len(key[0]) <= 2 or len(key[1]) <= 2:\\n        del away_corpus[key]\\n    elif key[0] in stopwords or key[1] in stopwords:\\n        del away_corpus[key]\\n    elif key[0][0] == '#' or key[1][0] == '#':\\n        del away_corpus[key]\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove words with <= 2 characters, stopwords, hashtags\n",
    "'''\n",
    "for key in list(home_corpus.keys()):\n",
    "    if len(key[0]) <= 2 or len(key[1]) <= 2:\n",
    "        del home_corpus[key]\n",
    "    elif key[0] in stopwords or key[1] in stopwords:\n",
    "        del home_corpus[key]\n",
    "    elif key[0][0] == '#' or key[1][0] == '#':\n",
    "        del home_corpus[key]\n",
    "\n",
    "for key in list(away_corpus.keys()):\n",
    "    if len(key[0]) <= 2 or len(key[1]) <= 2:\n",
    "        del away_corpus[key]\n",
    "    elif key[0] in stopwords or key[1] in stopwords:\n",
    "        del away_corpus[key]\n",
    "    elif key[0][0] == '#' or key[1][0] == '#':\n",
    "        del away_corpus[key]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turns a list of tweets (for one team, for one game)\n",
    "#into a vector using the top_grams\n",
    "from collections import Counter\n",
    "def vectorize_log(list_of_tweets, corpus):\n",
    "    num_tweets = len(list_of_tweets)\n",
    "    lol = [nltk.bigrams(x.split()) for x in list_of_tweets]\n",
    "    with_repeats = [item for sublist in lol for item in sublist]\n",
    "    counts = dict(Counter(with_repeats))\n",
    "    #print(counts)\n",
    "    to_return = []\n",
    "    for key in corpus:\n",
    "        num = np.log(1+counts[key]) if key in counts else 0\n",
    "        to_return.append(num)\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def vectorize_normalize(list_of_tweets, corpus):\n",
    "    num_tweets = len(list_of_tweets)\n",
    "    lol = [x.split() for x in list_of_tweets]\n",
    "    with_repeats = [item for sublist in lol for item in sublist]\n",
    "    counts = dict(Counter(with_repeats))\n",
    "    to_return = []\n",
    "    for key in corpus:\n",
    "        num = counts[key]/num_tweets if key in counts else 0\n",
    "        to_return.append(num)\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of tweets for all home and away teams\n",
    "n_home_tweets = sum([len(game_tweets) for game_tweets in home_tweets])\n",
    "n_away_tweets = sum([len(game_tweets) for game_tweets in away_tweets])\n",
    "\n",
    "# get the unigrams that appear in at least 0.1% of home/away tweets\n",
    "home_top_grams = [word for word in home_corpus if home_corpus[word] > n_home_tweets*0.001]\n",
    "away_top_grams = [word for word in away_corpus if away_corpus[word] > n_away_tweets*0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN SET\n",
    "#Create lists of vectors for home, away games\n",
    "num_h_tweets = [vectorize_log(game, home_top_grams) for game in home_tweets[:train_test_split]]\n",
    "num_a_tweets = [vectorize_log(game, away_top_grams) for game in away_tweets[:train_test_split]]\n",
    "\n",
    "#Turn into arrays\n",
    "home_vecs = np.array(num_h_tweets)\n",
    "away_vecs = np.array(num_a_tweets)\n",
    "\n",
    "#Concatenate home, away to form input matrix.\n",
    "X_train = np.concatenate([home_vecs, away_vecs], axis=1)\n",
    "\n",
    "# TEST SET\n",
    "#Create lists of vectors for home, away games\n",
    "num_h_tweets = [vectorize_log(game, home_top_grams) for game in home_tweets[train_test_split:]]\n",
    "num_a_tweets = [vectorize_log(game, away_top_grams) for game in away_tweets[train_test_split:]]\n",
    "\n",
    "#Turn into arrays\n",
    "home_vecs = np.array(num_h_tweets)\n",
    "away_vecs = np.array(num_a_tweets)\n",
    "\n",
    "#Concatenate home, away to form input matrix.\n",
    "X_test = np.concatenate([home_vecs, away_vecs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.array(season[\"Home Win\"])[:train_test_split]\n",
    "Y_test = np.array(season[\"Home Win\"])[train_test_split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((192, 1172), (192,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((64, 1172), (64,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa211cf9d90>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu', input_dim=X_train.shape[1]))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=50, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step - loss: 1.2428 - accuracy: 0.6719\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2428220510482788, 0.671875]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 7.7603 - accuracy: 0.5332\n",
      "Epoch 2/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 7.3335 - accuracy: 0.5635\n",
      "Epoch 3/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 6.9765 - accuracy: 0.6068\n",
      "Epoch 4/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 6.5781 - accuracy: 0.6650\n",
      "Epoch 5/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 6.2218 - accuracy: 0.6568\n",
      "Epoch 6/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 5.8389 - accuracy: 0.7283\n",
      "Epoch 7/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 5.4526 - accuracy: 0.8128\n",
      "Epoch 8/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 5.1340 - accuracy: 0.8188\n",
      "Epoch 9/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 4.8318 - accuracy: 0.8032\n",
      "Epoch 10/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 4.6024 - accuracy: 0.8167\n",
      "Epoch 11/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 4.3520 - accuracy: 0.8371\n",
      "Epoch 12/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 4.0688 - accuracy: 0.9252\n",
      "Epoch 13/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 3.8459 - accuracy: 0.9694\n",
      "Epoch 14/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 3.6452 - accuracy: 0.9764\n",
      "Epoch 15/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 3.4512 - accuracy: 0.9926\n",
      "Epoch 16/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 3.2960 - accuracy: 0.9965\n",
      "Epoch 17/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 3.1536 - accuracy: 1.0000\n",
      "Epoch 18/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 3.0179 - accuracy: 1.0000\n",
      "Epoch 19/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 2.9008 - accuracy: 1.0000\n",
      "Epoch 20/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.7932 - accuracy: 1.0000\n",
      "Epoch 21/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 2.6948 - accuracy: 1.0000\n",
      "Epoch 22/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.6014 - accuracy: 1.0000\n",
      "Epoch 23/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.5127 - accuracy: 1.0000\n",
      "Epoch 24/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.4316 - accuracy: 1.0000\n",
      "Epoch 25/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.3552 - accuracy: 1.0000\n",
      "Epoch 26/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.2811 - accuracy: 1.0000\n",
      "Epoch 27/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 2.2125 - accuracy: 1.0000\n",
      "Epoch 28/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.1466 - accuracy: 1.0000\n",
      "Epoch 29/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.0868 - accuracy: 1.0000\n",
      "Epoch 30/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.0321 - accuracy: 1.0000\n",
      "Epoch 31/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.9691 - accuracy: 1.0000\n",
      "Epoch 32/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.9153 - accuracy: 1.0000\n",
      "Epoch 33/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.8629 - accuracy: 1.0000\n",
      "Epoch 34/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.8143 - accuracy: 1.0000\n",
      "Epoch 35/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.7685 - accuracy: 1.0000\n",
      "Epoch 36/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 3.0544 - accuracy: 0.8271\n",
      "Epoch 37/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 2.6986 - accuracy: 0.6286\n",
      "Epoch 38/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 3.0108 - accuracy: 0.5277\n",
      "Epoch 39/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.9235 - accuracy: 0.5046\n",
      "Epoch 40/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.7355 - accuracy: 0.8501\n",
      "Epoch 41/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 2.6766 - accuracy: 0.6711\n",
      "Epoch 42/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 2.5350 - accuracy: 0.7540\n",
      "Epoch 43/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.4202 - accuracy: 0.7817\n",
      "Epoch 44/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 2.2646 - accuracy: 0.8611\n",
      "Epoch 45/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.1758 - accuracy: 0.8786\n",
      "Epoch 46/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 2.0525 - accuracy: 0.9291\n",
      "Epoch 47/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.9321 - accuracy: 0.9777\n",
      "Epoch 48/50\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 1.8683 - accuracy: 0.9455\n",
      "Epoch 49/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.7907 - accuracy: 0.9928\n",
      "Epoch 50/50\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 1.7002 - accuracy: 0.9928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa2113f0a60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='linear', kernel_regularizer=l1(.001), input_dim=X_train.shape[1]))\n",
    "model.add(LeakyReLU())\n",
    "#model.add(Dropout(.5))\n",
    "model.add(Dense(64, activation='linear',kernel_regularizer=l1(.001)))\n",
    "model.add(LeakyReLU())\n",
    "#model.add(Dropout(.5))\n",
    "model.add(Dense(64, activation='linear',kernel_regularizer=l1(.001)))\n",
    "model.add(LeakyReLU())\n",
    "#model.add(Dropout(.5))\n",
    "model.add(Dense(64, activation='linear',kernel_regularizer=l1(.001)))\n",
    "model.add(LeakyReLU())\n",
    "#model.add(Dropout(.5))\n",
    "model.add(Dense(32, activation='linear',kernel_regularizer=l1(.001)))\n",
    "model.add(LeakyReLU())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train, Y_train, epochs=50, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step - loss: 2.4141 - accuracy: 0.6875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.4141204357147217, 0.6875]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
