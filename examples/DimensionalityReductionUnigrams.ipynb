{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime\n",
    "import pickle\n",
    "import nltk\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in tweets\n",
    "with open('../data/pickled_tweets_corpus/4seasons_home_unigrams.pkl', 'rb') as f:\n",
    "    home_tweets = pickle.load(f)\n",
    "with open('../data/pickled_tweets_corpus/4seasons_away_unigrams.pkl', 'rb') as f:\n",
    "    away_tweets = pickle.load(f)\n",
    "\n",
    "# load in nfl data\n",
    "s1 = pd.read_csv('../data/season_data/2020_all_data.csv', index_col=0)\n",
    "s2 = pd.read_csv('../data/season_data/2019_all_data.csv', index_col=0)\n",
    "s3 = pd.read_csv('../data/season_data/2018_all_data.csv', index_col=0)\n",
    "s4 = pd.read_csv('../data/season_data/2017_all_data.csv', index_col=0)\n",
    "full = pd.concat([s4,s3,s2,s1],axis=0)\n",
    "full['Datetime'] = pd.DatetimeIndex(full['Datetime'], tz='US/Eastern')\n",
    "full['Datetime'] = full['Datetime'].dt.tz_convert('UTC')\n",
    "full['Home'] = np.where(full['Home'] == 'Washington Redskins', 'Washington Football Team', full['Home'])\n",
    "full['Away'] = np.where(full['Away'] == 'Washington Redskins', 'Washington Football Team', full['Away'])\n",
    "full['Home'] = np.where(full['Home'] == 'San Diego Chargers', 'Los Angeles Chargers', full['Home'])\n",
    "full['Away'] = np.where(full['Away'] == 'San Diego Chargers', 'Los Angeles Chargers', full['Away'])\n",
    "full['Home'] = np.where(full['Home'] == 'St Louis Rams', 'Los Angeles Rams', full['Home'])\n",
    "full['Away'] = np.where(full['Away'] == 'St Louis Rams', 'Los Angeles Rams', full['Away'])\n",
    "full['Home'] = np.where(full['Home'] == 'Oakland Raiders', 'Las Vegas Raiders', full['Home'])\n",
    "full['Away'] = np.where(full['Away'] == 'Oakland Raiders', 'Las Vegas Raiders', full['Away'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def vectorize_list(list_of_tweets, corpus):\n",
    "    num_tweets = len(list_of_tweets)\n",
    "    lol = [x.split() for x in list_of_tweets]\n",
    "    with_repeats = [item for sublist in lol for item in sublist]\n",
    "    counts = dict(Counter(with_repeats))\n",
    "    to_return = []\n",
    "    for key in corpus:\n",
    "        num = counts[key]/num_tweets if key in counts else 0\n",
    "        to_return.append(num)\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count unigrams\n",
    "def count_unigrams(tweet, corpus):\n",
    "    words = tweet.split(\" \")\n",
    "    for word in words:\n",
    "        if word in corpus:\n",
    "            corpus[word] += 1\n",
    "        else:\n",
    "            corpus[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the home/away corpus\n",
    "home_corpus = {}\n",
    "away_corpus = {}\n",
    "counter = 0\n",
    "for tweets in home_tweets:\n",
    "    if counter >= 768:\n",
    "        break\n",
    "    for tw in tweets:\n",
    "        count_unigrams(tw,home_corpus)\n",
    "    counter += 1\n",
    "\n",
    "counter = 0\n",
    "for tweets in away_tweets:\n",
    "    if counter >= 768:\n",
    "        break\n",
    "    for tw in tweets:\n",
    "        count_unigrams(tw,away_corpus)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a list of stopwords to remove.\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words with <= 2 characters, stopwords, hashtags\n",
    "for key in list(home_corpus.keys()):\n",
    "    if len(key) <= 2:\n",
    "        del home_corpus[key]\n",
    "    elif key in stopwords:\n",
    "        del home_corpus[key]\n",
    "    elif key[0] == '#':\n",
    "        del home_corpus[key]\n",
    "\n",
    "for key in list(away_corpus.keys()):\n",
    "    if len(key) <= 2:\n",
    "        del away_corpus[key]\n",
    "    elif key in stopwords:\n",
    "        del away_corpus[key]\n",
    "    elif key[0] == '#':\n",
    "        del away_corpus[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turns a list of tweets (for one team, for one game)\n",
    "#into a vector using the top_grams\n",
    "from collections import Counter\n",
    "def vectorize_list(list_of_tweets, corpus):\n",
    "    num_tweets = len(list_of_tweets)\n",
    "    lol = [x.split() for x in list_of_tweets]\n",
    "    with_repeats = [item for sublist in lol for item in sublist]\n",
    "    counts = dict(Counter(with_repeats))\n",
    "    to_return = []\n",
    "    for key in corpus:\n",
    "        num = np.log(1+counts[key]) if key in counts else 0\n",
    "        to_return.append(num)\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of tweets for all home and away teams\n",
    "n_home_tweets = sum([len(game_tweets) for game_tweets in home_tweets])\n",
    "n_away_tweets = sum([len(game_tweets) for game_tweets in away_tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unigrams that appear in at least 0.1% of home/away tweets\n",
    "home_top_grams = [word for word in home_corpus if home_corpus[word] > n_home_tweets*0.001]\n",
    "away_top_grams = [word for word in away_corpus if away_corpus[word] > n_away_tweets*0.001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN SET\n",
    "#Create lists of vectors for home, away games\n",
    "num_h_tweets = [vectorize_list(game, home_top_grams) for game in home_tweets[:768]]\n",
    "num_a_tweets = [vectorize_list(game, away_top_grams) for game in away_tweets[:768]]\n",
    "\n",
    "#Turn into arrays\n",
    "home_vecs = np.array(num_h_tweets)\n",
    "away_vecs = np.array(num_a_tweets)\n",
    "\n",
    "#Concatenate home, away to form input matrix.\n",
    "X_train = np.concatenate([home_vecs, away_vecs], axis=1)\n",
    "\n",
    "# TEST SET\n",
    "#Create lists of vectors for home, away games\n",
    "num_h_tweets = [vectorize_list(game, home_top_grams) for game in home_tweets[768:]]\n",
    "num_a_tweets = [vectorize_list(game, away_top_grams) for game in away_tweets[768:]]\n",
    "\n",
    "#Turn into arrays\n",
    "home_vecs = np.array(num_h_tweets)\n",
    "away_vecs = np.array(num_a_tweets)\n",
    "\n",
    "#Concatenate home, away to form input matrix.\n",
    "X_test = np.concatenate([home_vecs, away_vecs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.array(full[\"Home Win\"])[:768]\n",
    "Y_test = np.array(full[\"Home Win\"])[768:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5390625\n",
      "0.46875\n",
      "0.53125\n",
      "0.48046875\n",
      "0.5078125\n"
     ]
    }
   ],
   "source": [
    "# Basic classifiers on full data\n",
    "lr = LogisticRegression(penalty=\"l1\",solver=\"liblinear\",C=0.1)\n",
    "lr.fit(X_train, Y_train)\n",
    "print(lr.score(X_test, Y_test))\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators=1000)\n",
    "ada.fit(X_train, Y_train)\n",
    "print(ada.score(X_test, Y_test))\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, Y_train)\n",
    "print(rf.score(X_test, Y_test))\n",
    "\n",
    "mlp = MLPClassifier(max_iter=10000)\n",
    "mlp.fit(X_train, Y_train)\n",
    "print(mlp.score(X_test, Y_test))\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, Y_train)\n",
    "print(gnb.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 5 artists>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use PCA to reduce dimensions of X to 10 components\n",
    "n = 5\n",
    "pca = sklearn.decomposition.PCA(n_components=n)\n",
    "pca.fit(X_train)\n",
    "plt.bar(x=range(n),height=pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_X_train = pca.transform(X_train)\n",
    "reduced_X_test = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5598940533151058\n",
      "0.5427887901572113\n",
      "0.5000683526999316\n",
      "0.5300068352699931\n",
      "0.5468386876281613\n"
     ]
    }
   ],
   "source": [
    "# Basic classifiers on reduced data\n",
    "lr = LogisticRegression(penalty=\"l1\",solver=\"liblinear\",C=0.1)\n",
    "lr.fit(reduced_X_train, Y_train)\n",
    "print(np.mean(sklearn.model_selection.cross_val_score(lr,reduced_X_train,Y_train,cv=10)))\n",
    "\n",
    "ada = AdaBoostClassifier(n_estimators=5)\n",
    "ada.fit(reduced_X_train, Y_train)\n",
    "print(np.mean(sklearn.model_selection.cross_val_score(ada,reduced_X_train,Y_train,cv=10)))\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=10)\n",
    "rf.fit(reduced_X_train, Y_train)\n",
    "print(np.mean(sklearn.model_selection.cross_val_score(rf,reduced_X_train,Y_train,cv=10)))\n",
    "\n",
    "mlp = MLPClassifier(max_iter=10000)\n",
    "mlp.fit(reduced_X_train, Y_train)\n",
    "print(np.mean(sklearn.model_selection.cross_val_score(mlp,reduced_X_train,Y_train,cv=10)))\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(reduced_X_train, Y_train)\n",
    "print(np.mean(sklearn.model_selection.cross_val_score(gnb,reduced_X_train,Y_train,cv=10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
