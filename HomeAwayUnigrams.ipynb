{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the hashtag list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_hashes = {}\n",
    "with open('teams.nfl.hashtags.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        words = line.split()\n",
    "        team = words[0].replace('-', ' ')\n",
    "        hashes = words[1:]\n",
    "        team_hashes[team] = hashes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to get all hashtags from a given tweet; function below takes a tweet and a team name (that we want that tweet to only be associated with) and checks if it contains tweets from any other team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hashtags(tweet):\n",
    "    pattern = r'#{1}\\w*'\n",
    "    return re.findall(pattern, tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_team_only(one_tweet, which_team):\n",
    "    all_teams = team_hashes.copy()\n",
    "    good = all_teams.pop(which_team)\n",
    "    temp = [team_hashes[key] for key in all_teams]\n",
    "    bad_teams = [item for sublist in temp for item in sublist]\n",
    "    bad_teams = set(bad_teams)\n",
    "    curr_hashes = set(get_hashtags(one_tweet))\n",
    "    \n",
    "    if len(list(curr_hashes & bad_teams)) != 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the data for a full season, and convert the type of the datetime column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_season = pd.read_csv('2020_all_data.csv', index_col=0)\n",
    "full_season['Datetime'] = pd.to_datetime(full_season['Datetime'], utc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize containers. Home_tweets contains all tweets related to the home teams, same for away, and corpus contains all the unique words seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_tweets = []\n",
    "away_tweets = []\n",
    "corpus = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count unigrams\n",
    "def count_unigrams(tweet):\n",
    "    global corpus\n",
    "    words = tweet.split(\" \")\n",
    "    for word in words:\n",
    "        if word in corpus:\n",
    "            corpus[word] += 1\n",
    "        else:\n",
    "            corpus[word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Returns all valid tweets for one game, for one team. A valid tweet is one which is between the desired start and end times; has at least 10 likes; and only contains hashtags related to one team."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_game_one_team(game_time, team_name):\n",
    "    start = game_time - datetime.timedelta(days=1)\n",
    "    end = game_time - datetime.timedelta(hours=1)\n",
    "    date_range = \" since:\" + start.strftime('%Y-%m-%d') + \" until:\" + end.strftime('%Y-%m-%d')\n",
    "    \n",
    "    team_tags = \" OR \".join(team_hashes[team_name])\n",
    "    query = team_tags + date_range\n",
    "    \n",
    "    # Scrape the tweets for the date_range. Also have to filter based on the \n",
    "    # time stamp so as not to capture tweets during and after games.\n",
    "    to_return = []\n",
    "    for i,tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
    "        if i > 10000:\n",
    "            break\n",
    "        if not (start <= tweet.date and tweet.date <= end):\n",
    "            continue\n",
    "        if tweet.likeCount < 10:\n",
    "            continue\n",
    "        if not one_team_only(tweet.content.lower(), team_name):\n",
    "            continue\n",
    "        tw = tweet.content.lower()\n",
    "        to_return.append(tw)\n",
    "        count_unigrams(tw)\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gets all tweets for an entire season. Uses the above helper function to get a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season_tweets(df):\n",
    "    for game in range(len(df)):\n",
    "        gametime = df.iloc[game]['Datetime']\n",
    "        hometeam = df.iloc[game]['Home']\n",
    "        awayteam = df.iloc[game]['Away']\n",
    "        \n",
    "        h_tweets = one_game_one_team(gametime, hometeam)\n",
    "        a_tweets = one_game_one_team(gametime, awayteam)\n",
    "        \n",
    "        home_tweets.append(h_tweets)\n",
    "        away_tweets.append(a_tweets)\n",
    "        \n",
    "    return        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_season_tweets(full_season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/2020season_home.pkl', 'wb') as f:\n",
    "    pickle.dump(home_tweets, f)\n",
    "with open('2020season_away.pkl', 'wb') as f:\n",
    "    pickle.dump(away_tweets, f)\n",
    "with open('2020season_corpus.pkl', 'wb') as f:\n",
    "    pickle.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete words with less than 2 letters or less from the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "revised_corpus = corpus.copy()\n",
    "for key in list(revised_corpus.keys()):\n",
    "    if len(key) <= 2:\n",
    "        del revised_corpus[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the top 100 hundred unigrams by frequency. Call this our corpus (can be changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_grams = [x[0] for x in sorted(revised_corpus.items(), key=lambda x: x[1], reverse=True)[:3000]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that, given a list of (valid) tweets for one game and a corpus of interest, returns a numerical version representation of the game. Each number in the return vector represents one word in the corpus, and it is computed as the number of occurrences of that word in tweets about this game, divided by the total number of tweets about this game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def vectorize_list(list_of_tweets, corpus):\n",
    "    num_tweets = len(list_of_tweets)\n",
    "    lol = [x.split() for x in list_of_tweets]\n",
    "    with_repeats = [item for sublist in lol for item in sublist]\n",
    "    counts = dict(Counter(with_repeats))\n",
    "    to_return = []\n",
    "    for key in corpus:\n",
    "        num = counts[key]/num_tweets if key in counts else 0\n",
    "        to_return.append(num)\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.08333333333333333, 0.5, 0.16666666666666666, 0.08333333333333333]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_list(home_tweets[0], ['lots', 'of', 'a','at'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize each home and each away game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_h_tweets = [vectorize_list(game, top_grams) for game in home_tweets]\n",
    "num_a_tweets = [vectorize_list(game, top_grams) for game in away_tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate the home and away vectors as in the papers, and use this to produce our X and Y to perform models on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_vecs = np.array(num_h_tweets)\n",
    "away_vecs = np.array(num_a_tweets)\n",
    "X = np.concatenate([home_vecs, away_vecs], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = full_season.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 6000)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256,)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "everything = np.concatenate([X,Y.reshape(-1,1)], axis=1)\n",
    "np.random.shuffle(everything)\n",
    "xs = everything[:,:-1]\n",
    "ys = everything[:,-1]\n",
    "X_train = xs[:200]\n",
    "X_test = xs[200:]\n",
    "Y_train = ys[:200]\n",
    "Y_test = ys[200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, Y_train)\n",
    "lr.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada = AdaBoostClassifier(n_estimators=1000)\n",
    "ada.fit(X_train, Y_train)\n",
    "ada.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/X_2020_1000unigrams_no2letters', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('data/Y_2020_1000unigrams_no2letters', Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285714"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier().fit(X_train, Y_train)\n",
    "dt.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5178571428571429"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=1000)\n",
    "rf.fit(X_train, Y_train)\n",
    "rf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5178571428571429"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "mlp.fit(X_train, Y_train)\n",
    "mlp.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
